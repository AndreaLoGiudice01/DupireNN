{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dupireNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rQdgNr7Lt_rn",
        "0v-MujhxbB9L",
        "uix1XUaU8sa6",
        "gTX3rV8ka02z",
        "aVDBRx3BbcFW",
        "vNDkFJlYdi7h",
        "-qgxVPguB_6P",
        "KWdzg9DEv7eH",
        "s192l-r-B6lD",
        "hUMtwNuguMRu",
        "r9EvOdg0kU2P",
        "_eJG7_513hNC",
        "MrvDySFQWK-D",
        "NZBPdHPuB0CX",
        "NB3_-zutBt2y",
        "5yP7BP6avj5q",
        "nl9Gq8s4o_DH",
        "pMVAzDEKXe2Z",
        "s8feoYmarxmj",
        "3n8evyIxsB2W",
        "8ysLo9wxsTOj",
        "rRwNc1rgM28W",
        "_sdUIUb_MwVJ",
        "_RU2VfS9UcAN",
        "LIWdTVy8tf59",
        "z3Hrt5KOVt9S",
        "Pwupw4shWcqI",
        "Z45IZtNnWfek",
        "q-FhHmEUAaIj"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rQdgNr7Lt_rn"
      },
      "source": [
        "# Load modules and external files\n",
        "\n",
        "You need to import four python scripts for implied volatility calibration :\n",
        "- *newton.py*\n",
        "- *BSImplVol.py*\n",
        "- *BS.py*\n",
        "- *Bisect.py*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UbzsYb2nltYw",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.integrate as integrate\n",
        "from scipy import interpolate\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "from mpl_toolkits import mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import sklearn as skl\n",
        "from sklearn import preprocessing\n",
        "import importlib\n",
        "import scipy.stats as st\n",
        "import numpy as np\n",
        "import math\n",
        "import scipy.stats as st\n",
        "import matplotlib.ticker as mtick\n",
        "import time\n",
        "from scipy import interpolate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXQOZxW3tp7-",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5zOz93riuTb0",
        "colab": {}
      },
      "source": [
        "#Load python files to google colaborative environment\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-APUi7EiuTS7",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KySbpTpKuTO3",
        "colab": {}
      },
      "source": [
        "from BS import bsformula\n",
        "from Bisect import bisect\n",
        "from newton import newton\n",
        "from BSImplVol import bsimpvol"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ntAmGtpauTLK",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b8kSmrOxuTEw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdJ5B21juS5Y",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0v-MujhxbB9L"
      },
      "source": [
        "# Load data with google colab \n",
        "\n",
        "You will find in github repository six days of data.\n",
        "For each day you need to load six csv files :\n",
        "- *underlying.csv* for the stock value.\n",
        "- *locvol.csv* for the local volatility calibrated with tree pricing and tikhonov volatility (see Cr√©pey (2002)).\n",
        "- *dividend.csv* for dividend extracted from put-call parity.\n",
        "- *discount.csv* for zero-coupon curve. \n",
        "- *dataTrain.csv* for prices and/or implied volatility used in training set.\n",
        "- *dataTest.csv* for prices and/or implied volatility used in testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uSXibLknY4eB",
        "colab": {}
      },
      "source": [
        "#Load csv files to get data\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rq9GXpWFGEzP",
        "colab": {}
      },
      "source": [
        "#Read csv files as dataFrames\n",
        "zeroCouponCurve = pd.read_csv(\"discount.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "dividendCurve = pd.read_csv(\"dividend.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "trainingData = pd.read_csv(\"dataTrain.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "testingData = pd.read_csv(\"dataTest.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "underlyingNative = pd.read_csv(\"underlying.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "localVolatilityNative = pd.read_csv(\"locvol.csv\",decimal=\".\").apply(pd.to_numeric)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jPhQHR_tA_Jd",
        "colab": {}
      },
      "source": [
        "#Format dividend curve as a Pandas series\n",
        "dividendDf = dividendCurve.set_index('Maturity').sort_index()\n",
        "dividendDf.loc[1.0] = 0.0\n",
        "dividendDf.sort_index(inplace=True)\n",
        "dividendDf.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q8_eST11A_G0",
        "colab": {}
      },
      "source": [
        "#Format zero coupon curve as a Pandas series\n",
        "rateCurveDf = zeroCouponCurve.set_index('Maturity').sort_index()\n",
        "# keep only rates expriring before 1 year\n",
        "rateCurveDf = rateCurveDf.loc[rateCurveDf.index <= 1.01]\n",
        "rateCurveDf.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eP7776TtS4Ig",
        "colab": {}
      },
      "source": [
        "#Format local volatility\n",
        "localVolatility = localVolatilityNative.dropna()\n",
        "localVolatility[\"Strike\"] = localVolatility[\"stock(%)\"] * underlyingNative[\"S\"].values\n",
        "localVolatility[\"date\"] = localVolatility[\"date\"].round(decimals=3)\n",
        "renameDict = {\"date\": \"Maturity\", \n",
        "              \"vol\" : \"LocalVolatility\", \n",
        "              \"stock(%)\" : \"StrikePercentage\"}\n",
        "localVolatility = localVolatility.rename(columns=renameDict).set_index([\"Strike\", \"Maturity\"])\n",
        "localVolatility.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s3qMH7lzTas2",
        "colab": {}
      },
      "source": [
        "localVolatility.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQ-DlkP2GEus",
        "colab": {}
      },
      "source": [
        "underlyingNative.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IVaYoxqJic0M",
        "colab": {}
      },
      "source": [
        "testingData.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "arxEDvHu58sV",
        "colab": {}
      },
      "source": [
        "#Treatment for training data\n",
        "filteredTestingData = testingData[(testingData[\"Implied vol.\"] > 0) * (testingData[\"Option price\"] > 0)]\n",
        "filteredTestingData[\"Maturity\"] = filteredTestingData[\"Maturity\"].round(decimals=3)\n",
        "renameDict = {\"Implied vol.\": \"ImpliedVol\", \n",
        "              \"Option price\" : \"Price\", \n",
        "              \"Implied delta\" : \"ImpliedDelta\", \n",
        "              \"Implied gamma\" : \"ImpliedGamma\",\n",
        "              \"Implied theta\" : \"ImpliedTheta\",\n",
        "              \"Local delta\" : \"LocalDelta\",\n",
        "              \"Local gamma\" : \"LocalGamma\"}\n",
        "formattedTestingData = filteredTestingData.rename(columns=renameDict).set_index([\"Strike\", \"Maturity\"])[\"ImpliedVol\"]\n",
        "formattedTestingData.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xVtoYGxNicr5",
        "colab": {}
      },
      "source": [
        "trainingData.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V5sbhhcficoZ",
        "colab": {}
      },
      "source": [
        "#Treatment for testing data\n",
        "filteredTrainingData = trainingData[(trainingData[\"Calibrated\\nvol.\"] > 0) * (trainingData[\"Option\\nprice\"] > 0) * (trainingData[\"Option\\ntype\"] == 2)]\n",
        "filteredTrainingData[\"Maturity\"] = filteredTrainingData[\"Maturity\"].round(decimals=3)\n",
        "renameDict = {\"Option\\ntype\" : \"OptionType\", \n",
        "              \"Option\\nprice\" : \"Price\", \n",
        "              \"Calibrated\\nvol.\" : \"ImpliedVol\",#\"LocalImpliedVol\", \n",
        "              \"Implied\\nvol.\" : \"LocalImpliedVol\"}#\"ImpliedVol\"}\n",
        "formattedTrainingData = filteredTrainingData.drop([\"Active\", \"Market vol. -\\nCalibrated vol.\"],axis=1).rename(columns=renameDict).set_index([\"Strike\",\"Maturity\"])\n",
        "formattedTrainingData.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fico6X9Qick1",
        "colab": {}
      },
      "source": [
        "formattedTrainingData.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uix1XUaU8sa6"
      },
      "source": [
        "# Formatting data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VljyG0NV1CID"
      },
      "source": [
        "### Boostsrapping Rate Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wRo7rm_oS-Fm"
      },
      "source": [
        "\n",
        "- For bootstrapping short rate $r$ and dividend rate $q$, we assume piecewise constant short rate for risk free rate and dividend i.e. \n",
        "$\\exp{(-\\int_{0}^{T} r_t d_t)} = \\exp{(-\\sum_{i} r_i h)}$ and $\\exp{(\\int_{0}^{T} q_t d_t)} = \\exp{(\\sum_{i} q_i h)}$.\n",
        "- $\\forall i \\in \\{0,..,N\\}$ with $ t_0 = 0$ and $t_N = T$, we have that $\\frac{\\log{B(0,t_{i+1})} - \\log{B(0,t_i)}}{h} = r_i$ with $B(0,T_i)$ the price of a bond expiring at time $t_i$. \n",
        "- For dividend, we just to substitute $B(0,T_i)$ with with spot action price plus dividend cash flow received until time $T_i$ i.e. $S_{t_0} + \\sum\\limits_i Div_{t_i}$.\n",
        "- Then we linearly interpolate $r$ and $q$.\n",
        "-  Linear interpolation is also used for integrals $\\int_{0}^{T} q_t d_t$ and $\\int_{0}^{T} r_t d_t$ in order to obtain discount factor or dividend factor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5qXMpZsnmeKv",
        "colab": {}
      },
      "source": [
        "#Compute the integral and return the linear interpolation function \n",
        "def interpIntegral(curve):\n",
        "    #curve is piece-wise constant\n",
        "    timeDelta = curve.index.to_series().diff().fillna(0)\n",
        "    timeStep = np.linspace(0,0.99,100)\n",
        "    integralStepWise = (curve * timeDelta).cumsum()\n",
        "    integralStepWise.loc[0] = 0.0\n",
        "    integralStepWise.sort_index(inplace=True)\n",
        "    integralSpline = interpolate.interp1d(integralStepWise.index,\n",
        "                                          integralStepWise, \n",
        "                                          fill_value= 'extrapolate', \n",
        "                                          kind ='linear')\n",
        "    return pd.Series(integralSpline(timeStep),index=timeStep), integralSpline\n",
        "\n",
        "def bootstrapZeroCoupon(curvePrice, name):\n",
        "    #Bootstrap short rate curve\n",
        "    def computeShortRate(curve) :\n",
        "      shortRateList = [] \n",
        "      for i in range(curve.size):\n",
        "        if i == 0 :\n",
        "          shortRateList.append(-(np.log(curve.iloc[i]))/(curve.index[i]))\n",
        "        else : \n",
        "          shortRateList.append(-(np.log(curve.iloc[i])-np.log(curve.iloc[i-1]))/(curve.index[i]-curve.index[i-1]))\n",
        "      return pd.Series(shortRateList,index = curve.index)\n",
        "    #For t=0 we take the first available point to ensure right continuity\n",
        "    riskFreeCurve = computeShortRate(curvePrice)\n",
        "    riskFreeCurve.loc[0.00] = riskFreeCurve.iloc[0]\n",
        "    riskFreeCurve = riskFreeCurve.sort_index()\n",
        "\n",
        "    #Bootstrap yield curve\n",
        "    def zeroYield(x):\n",
        "      if(float(x.name) < 1):\n",
        "        return (1/x - 1)/float(x.name)\n",
        "      else:\n",
        "        return (x**(-1/float(x.name)) - 1)\n",
        "    yieldCurve = curvePrice.apply(zeroYield, axis = 1)\n",
        "    yieldCurve.loc[0.00] = yieldCurve.iloc[0]\n",
        "    yieldCurve = yieldCurve.sort_index()\n",
        "\n",
        "    plt.plot(riskFreeCurve, label = \"Short rate\")\n",
        "\n",
        "    #Interpolate short rate curve and yield curve\n",
        "    timeStep = np.linspace(0,0.99,100)\n",
        "    riskCurvespline = interpolate.interp1d(riskFreeCurve.index,\n",
        "                                           riskFreeCurve,#riskFreeCurve[name],\n",
        "                                           fill_value= 'extrapolate',\n",
        "                                           kind ='next')\n",
        "    interpolatedCurve = pd.Series(riskCurvespline(timeStep),index=timeStep)\n",
        "    plt.plot(interpolatedCurve, label=\"Interpolated short rate\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(yieldCurve, label = \"Yield curve\")\n",
        "    yieldCurvespline = interpolate.interp1d(yieldCurve.index,\n",
        "                                            yieldCurve['Price'],\n",
        "                                            fill_value= 'extrapolate',\n",
        "                                            kind ='next')\n",
        "    interpolatedCurve = pd.Series(yieldCurvespline(timeStep),index=timeStep)\n",
        "    plt.plot(interpolatedCurve, label = \"Interpolated Yield curve\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    #Integrate short rate\n",
        "    interpolatedIntegral, riskFreeIntegral = interpIntegral(riskFreeCurve)\n",
        "    plt.plot(interpolatedIntegral)\n",
        "    plt.show()\n",
        "\n",
        "    return riskFreeCurve, riskCurvespline, yieldCurve, yieldCurvespline, interpolatedIntegral, riskFreeIntegral\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-ahnsiAH9xh",
        "colab": {}
      },
      "source": [
        "riskFreeCurve, riskCurvespline, yieldCurve, yieldCurvespline, interpolatedIntegral, riskFreeIntegral = bootstrapZeroCoupon(rateCurveDf, \"Short rate\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkDf8dPPOW3Y",
        "colab": {}
      },
      "source": [
        "riskFreeCurve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yr0_fpL6OWdf",
        "colab": {}
      },
      "source": [
        "interpolatedIntegral"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JB4SRjCP1Wtv"
      },
      "source": [
        "### Boostraping dividend curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aawmDPX0EBMD",
        "colab": {}
      },
      "source": [
        "def bootstrapDividend(curvePrice, underlying, name):\n",
        "    #Compute cumulative sum of dividend plus spot price\n",
        "    priceEvolution = underlying['S'].iloc[0] - curvePrice['Amount'].cumsum()\n",
        "    priceEvolution.loc[0] = underlying['S'].iloc[0]\n",
        "    priceEvolution.sort_index(inplace=True)\n",
        "\n",
        "    #Bootstrap short rate for dividend\n",
        "    def computeShortRate(curve) :\n",
        "      shortRateList = [] \n",
        "      for i in range(curve.size):\n",
        "        if i == 0 :\n",
        "          shortRateList.append(-(np.log(curve.iloc[i+1])-np.log(curve.iloc[i]))/(curve.index[i+1]-curve.index[i]))\n",
        "        else : \n",
        "          shortRateList.append(-(np.log(curve.iloc[i])-np.log(curve.iloc[i-1]))/(curve.index[i]-curve.index[i-1]))\n",
        "      return pd.Series(shortRateList,index = curve.index).dropna()\n",
        "    logReturnDividendDf = computeShortRate(priceEvolution)\n",
        "\n",
        "    #Dividend yield curve\n",
        "    def divYield(x):\n",
        "      return ((priceEvolution[x]/priceEvolution.iloc[0])**(1/float(x)) - 1) #np.log(priceEvolution[x]/priceEvolution.iloc[0])/x\n",
        "    dividendYield = logReturnDividendDf.index.to_series().tail(-1).apply(divYield)\n",
        "    dividendYield.loc[0.00] = dividendYield.iloc[0]\n",
        "    dividendYield = dividendYield.sort_index()\n",
        "\n",
        "    plt.plot(logReturnDividendDf, label = \"Short rate\")\n",
        "\n",
        "    #Interpolate short rate curve and yield curve\n",
        "    timeStep = np.linspace(0,0.99,100)\n",
        "    logReturnDividendSpline = interpolate.interp1d(logReturnDividendDf.index,\n",
        "                                                   logReturnDividendDf,#logReturnDividendDf[name],\n",
        "                                                   fill_value= 'extrapolate',\n",
        "                                                   kind ='next')\n",
        "    interpolatedCurve = pd.Series(logReturnDividendSpline(timeStep),index=timeStep)\n",
        "    plt.plot(interpolatedCurve, label=\"Interpolated short rate\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(dividendYield, label = \"Yield curve\")\n",
        "    yieldCurvespline = interpolate.interp1d(dividendYield.index,\n",
        "                                            dividendYield.values,\n",
        "                                            fill_value= 'extrapolate',\n",
        "                                            kind ='next')\n",
        "    interpolatedCurve = pd.Series(yieldCurvespline(timeStep),index=timeStep)\n",
        "    plt.plot(interpolatedCurve, label = \"Interpolated Yield curve\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    #Integrate short rate\n",
        "    interpolatedIntegral, logReturnDividendIntegral = interpIntegral(logReturnDividendDf)#logReturnDividendDf[name])\n",
        "    plt.plot(interpolatedIntegral)\n",
        "    plt.show()\n",
        "\n",
        "    return logReturnDividendDf, logReturnDividendSpline, dividendYield, yieldCurvespline, interpolatedIntegral, logReturnDividendIntegral"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgQ9b3rpbCBl",
        "colab": {}
      },
      "source": [
        "spreadDividend, divSpline, yieldDividend, divYieldSpline, interpolatedIntegral, divSpreadIntegral  = bootstrapDividend(dividendDf, underlyingNative, \"Spread\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KJonVnKvOmfN",
        "colab": {}
      },
      "source": [
        "spreadDividend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-50wFPo7OmWP",
        "colab": {}
      },
      "source": [
        "interpolatedIntegral"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ucimXKUjVzDE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OlfrW4Gn2N4B"
      },
      "source": [
        "### Pricing black-scholes price\n",
        "\n",
        "#### Change of variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J5RE5f5UaTUT"
      },
      "source": [
        "- In presence of dividend rate $d$ and risk free rate $r$ Dupire formula is :   $$\\sigma^2(T,K) = 2 \\frac{ \\partial_T P(T,K) + (r-q)\\partial_K P(T,K) + qP(T,K)}{K¬≤ \\partial_{K}^2 P(T,K)}$$ \n",
        "with Strike $K$, Maturity $T$, dividend rate $q$ and risk-free rate $r$, $P$ our pricing function. \n",
        "- We apply the following change of variable : $$ w(T,k) = \\exp{(\\int_{0}^{T} q_t dt)} P(T,K)$$ with $K = k \\exp{(\\int_{0}^{T} (r_t - q_t) dt)} $.\n",
        "\n",
        "- Then Dupire equation becomes :  $\\sigma^2(T,K) = 2 \\frac{ \\partial_T w(T,k)}{k¬≤ \\partial_{k}^2 w(T,k)}$. \n",
        "- If we learn the mapping $v$ with a neural network then we should obtain quickly by adjoint differentiation $\\partial_T w$ and $\\partial_{k¬≤}^2 w$ and therefore $\\sigma$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5Zydj3k5Y-d",
        "colab": {}
      },
      "source": [
        "import scipy.stats as st\n",
        "#Density derivative\n",
        "def dpdf(x):\n",
        "    v = 1\n",
        "    return -x*np.exp(-x**2/(2.0*v**2))/(v**3*np.sqrt(2.0*np.pi))\n",
        "    \n",
        "\n",
        "def generalizedGreeks(cp, s, k, rf, t, v, div, rfInt, divInt):\n",
        "        \"\"\" Price an option using the Black-Scholes model.\n",
        "        cp: +1/-1 for call/put\n",
        "        s: initial stock price\n",
        "        k: strike price\n",
        "        t: expiration time\n",
        "        v: volatility\n",
        "        rf: risk-free rate at time t\n",
        "        div: dividend at time t\n",
        "        rfInt: deterministic risk-free rate integrated between 0 and t\n",
        "        divInt: deterministic dividend integrated between 0 and t\n",
        "        \"\"\"\n",
        "\n",
        "        d1 = (np.log(s/k)+(rfInt-divInt+0.5*v*v*t))/(v*np.sqrt(t))\n",
        "        d2 = d1 - v*np.sqrt(t)\n",
        "        \n",
        "        Nd1 = st.norm.cdf(cp*d1)\n",
        "        Nd2 = st.norm.cdf(cp*d2)\n",
        "\n",
        "        discountFactor = np.exp(-rfInt)\n",
        "        forwardFactor = np.exp(-divInt)\n",
        "        avgDiv = divInt/t\n",
        "        avgRf = rfInt/t\n",
        "\n",
        "        optprice = (cp*s*forwardFactor*Nd1) - (cp*k*discountFactor*Nd2)\n",
        "\n",
        "        delta = cp*Nd1\n",
        "        vega  = s*np.sqrt(t)*st.norm.pdf(d1)\n",
        "        delta_k = -s*forwardFactor*Nd1/(v*np.sqrt(t)*k) - cp*discountFactor*Nd2 + k*discountFactor*Nd2/(v*np.sqrt(t)*k)\n",
        "        \n",
        "        gamma_k = s*forwardFactor/((v*np.sqrt(t)*k)**2)*(Nd1*v*np.sqrt(t) + cp*dpdf(cp*d1)) - k*discountFactor/((v*np.sqrt(t)*k)**2)*(Nd2*v*np.sqrt(t) + cp*dpdf(cp*d2)) +  2.0*discountFactor*Nd2/(v*np.sqrt(t)*k)  \n",
        "\n",
        "        dd1_dt = (avgRf-avgDiv+0.5*v*v)/(v*np.sqrt(t)) - 0.5*(np.log(s/k)+(rfInt-divInt+0.5*v*v*t))/(v*v*t**(3/2))\n",
        "        dd2_dt = dd1_dt - 0.5*v/np.sqrt(t)\n",
        "        delta_T = avgRf*cp*k*discountFactor*Nd2 - avgDiv*cp*s*forwardFactor*Nd1 + s*forwardFactor*Nd1*dd1_dt- k*discountFactor*Nd2*dd2_dt\n",
        "        \n",
        "        return optprice, delta, vega, delta_k, gamma_k, delta_T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HbOR7gse5Y4x",
        "colab": {}
      },
      "source": [
        "S0 = underlyingNative[\"S\"].values\n",
        "#Change of variable for deterministic discount curve and dividend curve\n",
        "def changeOfVariable(s,t):\n",
        "  def qInterp(m):\n",
        "    return divSpreadIntegral(m).astype(np.float32)\n",
        "  q = qInterp(t)\n",
        "  \n",
        "  def rInterp(m):\n",
        "    return riskFreeIntegral(m).astype(np.float32)\n",
        "  r = rInterp(t)\n",
        "\n",
        "  factorPrice = np.exp( - q )\n",
        "\n",
        "  divSpread = q-r\n",
        "\n",
        "  factorStrike = np.exp( divSpread )\n",
        "  adjustedStrike = np.multiply(s, factorStrike)\n",
        "  return adjustedStrike, factorPrice\n",
        "\n",
        "#Change of variable for constant discount and dividend short rate \n",
        "def changeOfVariable_BS(s,t):\n",
        "  \n",
        "  factorPrice = np.exp( - q*t )\n",
        "\n",
        "  divSpread = (q-r)*t\n",
        "\n",
        "  factorStrike = np.exp( divSpread )\n",
        "  adjustedStrike = np.multiply(s, factorStrike)\n",
        "  return adjustedStrike, factorPrice\n",
        "\n",
        "#Generate a proper dataset from implied volatility\n",
        "def generateTestingData(impliedVol, S0, rIntegralSpline, qIntegralSpline, rSpline, qSpline):\n",
        "\n",
        "  x_train = impliedVol.index.to_frame()\n",
        "  x_train[\"ImpliedVol\"] = impliedVol\n",
        "  isPut = True\n",
        "  cp = -1 if isPut else 1\n",
        "  impliedPriceFunction = lambda x : generalizedGreeks(cp, \n",
        "                                                      S0, \n",
        "                                                      x[\"Strike\"] , \n",
        "                                                      rSpline(x[\"Maturity\"]), \n",
        "                                                      x[\"Maturity\"], \n",
        "                                                      x[\"ImpliedVol\"], \n",
        "                                                      qSpline(x[\"Maturity\"]), \n",
        "                                                      rIntegralSpline(x[\"Maturity\"]), \n",
        "                                                      qIntegralSpline(x[\"Maturity\"]))\n",
        "  \n",
        "  res = np.reshape(np.array(list(zip(x_train.apply(impliedPriceFunction,axis=1).values))),(x_train.shape[0], 6))  # put greeks\n",
        "  prices = res[:,0]\n",
        "  deltas = res[:,1]\n",
        "  vegas = res[:,2]\n",
        "  delta_ks = res[:,3]\n",
        "  gamma_ks = res[:,4]\n",
        "  delta_Ts = res[:,5]\n",
        "  \n",
        "  sigmaRef = 0.25\n",
        "  impliedPriceFunction = lambda x : generalizedGreeks(cp, \n",
        "                                                      S0, \n",
        "                                                      x[\"Strike\"] , \n",
        "                                                      rSpline(x[\"Maturity\"]), \n",
        "                                                      x[\"Maturity\"], \n",
        "                                                      sigmaRef, \n",
        "                                                      qSpline(x[\"Maturity\"]), \n",
        "                                                      rIntegralSpline(x[\"Maturity\"]), \n",
        "                                                      qIntegralSpline(x[\"Maturity\"]))\n",
        "  \n",
        "  res1 = np.reshape(np.array(list(zip(x_train.apply(impliedPriceFunction,axis=1).values))),(x_train.shape[0], 6))  # put greeks\n",
        "\n",
        "  changedVar = changeOfVariable(x_train[\"Strike\"],x_train[\"Maturity\"])\n",
        "  \n",
        "  multiIndex = impliedVol.index\n",
        "  \n",
        "  cols = [\"Price\", \"Delta\", \"Vega\", \"Delta Strike\", \"Gamma Strike\", \n",
        "          \"Theta\", \"ChangedStrike\", \"DividendFactor\", \"Strike\", \"Maturity\", \"ImpliedVol\", \"VegaRef\"]\n",
        "\n",
        "  dfData = np.vstack((prices, deltas, vegas, delta_ks, gamma_ks, delta_Ts) + \n",
        "                     changedVar + (x_train[\"Strike\"], x_train[\"Maturity\"], x_train[\"ImpliedVol\"], res1[:,2]))\n",
        "  df = pd.DataFrame(dfData.T , columns=cols, index = multiIndex)\n",
        "  \n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3sQVozdp5Y1p",
        "colab": {}
      },
      "source": [
        "testingDataSet = generateTestingData(formattedTestingData, S0, riskFreeIntegral, divSpreadIntegral, riskCurvespline, divSpline)\n",
        "testingDataSet.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c3Do8EBH5YnO",
        "colab": {}
      },
      "source": [
        "#Checking call put parity\n",
        "maturity = testingData.iloc[-4][\"Maturity\"]\n",
        "strike = testingData.iloc[-4][\"Strike\"]\n",
        "(S0 * np.exp(-divSpreadIntegral(maturity))  - np.exp(-riskFreeIntegral(maturity)) * strike) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmAkv50Td1dJ",
        "colab": {}
      },
      "source": [
        "testingData.iloc[-4][\"Option price\"] - testingDataSet.iloc[-4][\"Price\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h0ag0ChkGEkP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KNvpFAU1FN78",
        "colab": {}
      },
      "source": [
        "trainingDataSet = generateTestingData(formattedTrainingData[\"ImpliedVol\"], S0, riskFreeIntegral, divSpreadIntegral, riskCurvespline, divSpline)\n",
        "trainingDataSet.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0TDRVwgkFN4c",
        "colab": {}
      },
      "source": [
        "filteredTrainingData.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WCkjHBiSFN2D",
        "colab": {}
      },
      "source": [
        "filteredTrainingData[filteredTrainingData[\"Strike\"]==5900]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xj_4dnWpFNul",
        "colab": {}
      },
      "source": [
        "trainingDataSet[trainingDataSet[\"Strike\"]==5900]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z5u31XfRfe5s",
        "colab": {}
      },
      "source": [
        "filteredTrainingData[filteredTrainingData[\"Strike\"]==4000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1bGr3ZOPfev0",
        "colab": {}
      },
      "source": [
        "trainingDataSet[trainingDataSet[\"Strike\"]==4000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lC01cOOrB6kE",
        "colab": {}
      },
      "source": [
        "localVolatility.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5oBekB4C5YyD",
        "colab": {}
      },
      "source": [
        "testingData.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ttVRKskQEi4g",
        "colab": {}
      },
      "source": [
        "#Get local volatility from Cr√©pey (2002) by nearest neighbour interpolation\n",
        "def interpolatedLocalVolatility(localVol, priceGrid):\n",
        "    strikeLocVol = np.ravel(localVol.index.get_level_values(\"Strike\").values)\n",
        "    maturityLocVol = np.ravel(localVol.index.get_level_values(\"Maturity\").values)\n",
        "    \n",
        "    xym = np.vstack((strikeLocVol, maturityLocVol)).T\n",
        "    opts = {'balanced_tree': False, 'compact_nodes': False}\n",
        "    f =  interpolate.NearestNDInterpolator(xym,\n",
        "                                           localVol[\"LocalVolatility\"].values.flatten(), \n",
        "                                           rescale=True,\n",
        "                                           tree_options=opts)\n",
        "\n",
        "    strikePrice = priceGrid.index.get_level_values(\"Strike\").values\n",
        "    maturityPrice = priceGrid.index.get_level_values(\"Maturity\").values\n",
        "    func = lambda x : f(x[0],x[1])\n",
        "    coordinates =  np.array( list( map( func, zip(strikePrice, maturityPrice) ) ) ).flatten() \n",
        "\n",
        "    return pd.Series(coordinates, index = priceGrid.index)\n",
        "\n",
        "trainingDataSet[\"locvol\"] = interpolatedLocalVolatility(localVolatility, trainingDataSet[\"Price\"])\n",
        "testingDataSet[\"locvol\"] = interpolatedLocalVolatility(localVolatility, testingDataSet[\"Price\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wyOafgBuKk1d",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QUxTjD2zY706",
        "colab": {}
      },
      "source": [
        "dataSet = trainingDataSet #Training set\n",
        "dataSetTest = testingDataSet #Testing set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RyOtl8iCziXl"
      },
      "source": [
        "# Neural network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTX3rV8ka02z"
      },
      "source": [
        "## Scaling methods\n",
        "\n",
        "Use min-max of scaling strike between 0 et 1 for improving stability of neural network training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLqa33lNSXTB",
        "colab": {}
      },
      "source": [
        "def transformCustomMinMax(df, scaler):\n",
        "  return pd.DataFrame(scaler.transform(df),\n",
        "                      index = df.index, \n",
        "                      columns = df.columns)\n",
        "#Reverse operation min-max scaling\n",
        "def inverseTransformMinMax(df, scaler):\n",
        "  return pd.DataFrame(scaler.inverse_transform(df),\n",
        "                      index = df.index, \n",
        "                      columns = df.columns)\n",
        "#Same thing but for a particular column\n",
        "def inverseTransformColumnMinMax(originalDf, scaler, column):\n",
        "  colIndex = originalDf.columns.get_loc(column.name)\n",
        "  maxCol = scaler.data_max_[colIndex]\n",
        "  minCol = scaler.data_min_[colIndex]\n",
        "  return pd.Series(minCol + (maxCol - minCol) * column, index = column.index).rename(column.name)  \n",
        "#Reverse transform of min-max scaling but for greeks   \n",
        "def inverseTransformColumnGreeksMinMax(originalDf, \n",
        "                                       scaler,\n",
        "                                       columnDerivative,\n",
        "                                       columnFunctionName,\n",
        "                                       columnVariableName,\n",
        "                                       order = 1):\n",
        "  colFunctionIndex = originalDf.columns.get_loc(columnFunctionName)\n",
        "  maxColFunction = scaler.data_max_[colFunctionIndex]\n",
        "  minColFunction = scaler.data_min_[colFunctionIndex]\n",
        "  scaleFunction = (maxColFunction - minColFunction)\n",
        "  \n",
        "  colVariableIndex = originalDf.columns.get_loc(columnVariableName)\n",
        "  maxColVariable = scaler.data_max_[colVariableIndex]\n",
        "  minColVariable = scaler.data_min_[colVariableIndex]\n",
        "  scaleVariable = (maxColVariable - minColVariable) ** order\n",
        "\n",
        "  return pd.Series(scaleFunction * columnDerivative / scaleVariable , \n",
        "                   index = columnDerivative.index).rename(columnDerivative.name) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZFGpjMuoaJOh",
        "colab": {}
      },
      "source": [
        "#Tools functions for min-max scaling\n",
        "def transformCustomId(df, scaler):\n",
        "  return pd.DataFrame(df,\n",
        "                      index = df.index, \n",
        "                      columns = df.columns)\n",
        "def inverseTransformId(df, scaler):\n",
        "  return pd.DataFrame(df,\n",
        "                      index = df.index, \n",
        "                      columns = df.columns)\n",
        "def inverseTransformColumnId(originalDf, scaler, column):\n",
        "  return pd.Series(column, index = column.index).rename(column.name)  \n",
        "\n",
        "def inverseTransformColumnGreeksId(originalDf, scaler, \n",
        "                                 columnDerivative, \n",
        "                                 columnFunctionName, \n",
        "                                 columnVariableName,\n",
        "                                 order = 1):\n",
        "  return pd.Series(columnDerivative , index = columnDerivative.index).rename(columnDerivative.name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XkPnYeFSbWKt",
        "colab": {}
      },
      "source": [
        "activateScaling = False\n",
        "transformCustom = transformCustomMinMax if activateScaling else transformCustomId\n",
        "inverseTransform = inverseTransformMinMax if activateScaling else inverseTransformId\n",
        "inverseTransformColumn = inverseTransformColumnMinMax if activateScaling else inverseTransformColumnId\n",
        "inverseTransformColumnGreeks = inverseTransformColumnGreeksMinMax if activateScaling else inverseTransformColumnGreeksId"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "83ZlEZ7Ys0tA",
        "colab": {}
      },
      "source": [
        "scaler = skl.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "scaler.fit(dataSet)\n",
        "scaledDataSet = transformCustom(dataSet, scaler)\n",
        "scaledDataSetTest = transformCustom(dataSetTest, scaler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sLjz7YjBw7o5",
        "colab": {}
      },
      "source": [
        "scaledDataSet.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lAUxbEzOnLMh",
        "colab": {}
      },
      "source": [
        "#Search strike for ATM option\n",
        "midS0 = dataSet[dataSet.index.get_level_values(\"Strike\") >= S0[0]].index.get_level_values(\"Strike\").min()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aVDBRx3BbcFW"
      },
      "source": [
        "## Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6McrN8zK6WL2",
        "colab": {}
      },
      "source": [
        "#Plot loss for each epoch \n",
        "def plotEpochLoss(lossSerie):\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax = fig.gca()\n",
        "  \n",
        "  ax.plot(lossSerie , \"-\", color=\"black\")\n",
        "  ax.set_xlabel(\"Epoch number\", fontsize=18, labelpad=20)\n",
        "  ax.set_ylabel(\"Logarithmic Loss\", fontsize=18, labelpad=20)\n",
        "  ax.set_title(\"Training Loss evolution\", fontsize=24)\n",
        "  ax.tick_params(labelsize=16)\n",
        "  ax.set_facecolor('white')\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DTXqhgHOSP7r",
        "colab": {}
      },
      "source": [
        "KMin = 0.7 * S0[0]\n",
        "KMax = 1.3 * S0[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kDz4FSbcprg7",
        "colab": {}
      },
      "source": [
        "#Plot a surface as a superposition of curves\n",
        "def plotMultipleCurve(data,\n",
        "                      Title = 'True Price Surface',\n",
        "                      yMin = KMin,\n",
        "                      yMax = KMax,\n",
        "                      zAsPercent = False):\n",
        "  \n",
        "\n",
        "  dataCurve = data[(data.index.get_level_values(\"Strike\") <= yMax) * (data.index.get_level_values(\"Strike\") >= yMin)]\n",
        "\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax = fig.gca()\n",
        "\n",
        "  for t in np.linspace(0,0.8,9) :\n",
        "    k = dataCurve[dataCurve.index.get_level_values(\"Maturity\") >= t].index.get_level_values(\"Maturity\").unique().min()\n",
        "    curveK = dataCurve[dataCurve.index.get_level_values(\"Maturity\")==k]\n",
        "    dataSerie = pd.Series(curveK.values * (100 if zAsPercent else 1) ,\n",
        "                          index = curveK.index.get_level_values(\"Strike\"))\n",
        "    ax.plot(dataSerie , \"--+\", label=str(k))\n",
        "  ax.legend()  \n",
        "  ax.set_xlabel(data.index.names[0], fontsize=18, labelpad=20)\n",
        "  ax.set_ylabel(data.name, fontsize=18, labelpad=20)\n",
        "  if zAsPercent :\n",
        "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "  ax.set_title(Title, fontsize=24)\n",
        "  ax.tick_params(labelsize=16)\n",
        "  ax.set_facecolor('white')\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NrzucF2AQznx",
        "colab": {}
      },
      "source": [
        "plotMultipleCurve(localVolatility[\"LocalVolatility\"][localVolatility.index.get_level_values(\"Maturity\")>0.01],\n",
        "                  Title = 'Local Volatility Surface',\n",
        "                  yMin=0.7*S0[0],\n",
        "                  yMax=1.4*S0[0], \n",
        "                  zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZYhSZTjM2Lqp",
        "colab": {}
      },
      "source": [
        "#Plotting function for surface\n",
        "#xTitle : title for x axis\n",
        "#yTitle : title for y axis\n",
        "#zTitle : title for z axis\n",
        "#Title : plot title\n",
        "#az : azimuth i.e. angle of view for surface\n",
        "#yMin : minimum value for y axis\n",
        "#yMax : maximum value for y axis\n",
        "#zAsPercent : boolean, if true format zaxis as percentage \n",
        "def plotGridCustom(coordinates, zValue,\n",
        "                   xTitle = \"Maturity\",\n",
        "                   yTitle = \"Strike\",\n",
        "                   zTitle = \"Price\",\n",
        "                   Title = 'True Price Surface', \n",
        "                   az=320, \n",
        "                   yMin = KMin,\n",
        "                   yMax = KMax,\n",
        "                   zAsPercent = False):\n",
        "  y = coordinates[:,0]\n",
        "  filteredValue = (y > yMin) & (y < yMax)\n",
        "  x = coordinates[:,1][filteredValue]\n",
        "  y = coordinates[:,0][filteredValue]\n",
        "  z = zValue[filteredValue].flatten()\n",
        "  \n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax = fig.gca(projection='3d')\n",
        "  \n",
        "  ax.set_xlabel(xTitle, fontsize=18, labelpad=20)\n",
        "  ax.set_ylabel(yTitle, fontsize=18, labelpad=20)\n",
        "  ax.set_zlabel(zTitle, fontsize=18, labelpad=10)\n",
        "  \n",
        "  cmap=plt.get_cmap(\"inferno\")\n",
        "  colors=cmap(z * 100 if zAsPercent else z)[np.newaxis, :, :3]\n",
        "  surf = ax.plot_trisurf(x, y,\n",
        "                         z * 100 if zAsPercent else z ,\n",
        "                         linewidth=1.0,\n",
        "                         antialiased=True, \n",
        "                         cmap = cmap,\n",
        "                         color=(0,0,0,0))\n",
        "  scaleEdgeValue = surf.to_rgba(surf.get_array())\n",
        "  surf.set_edgecolors(scaleEdgeValue) \n",
        "  surf.set_alpha(0)\n",
        "\n",
        "  if zAsPercent :\n",
        "    ax.zaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "  ax.view_init(elev=10., azim=az)\n",
        "  ax.set_title(Title, fontsize=24)\n",
        "  ax.set_facecolor('white')\n",
        "\n",
        "  plt.tick_params(labelsize=16)\n",
        "\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTNEnEsWbBOE",
        "colab": {}
      },
      "source": [
        "#Plotting function from a dataframe\n",
        "def plotSurface(data, \n",
        "                zName, \n",
        "                Title = 'True Price Surface', \n",
        "                az=320,\n",
        "                yMin = KMin,\n",
        "                yMax = KMax,\n",
        "                zAsPercent = False):\n",
        "  plotGridCustom(dataSet.index.to_frame().values, \n",
        "                 data[zName].values,\n",
        "                 xTitle = dataSet.index.names[1],\n",
        "                 yTitle = dataSet.index.names[0],\n",
        "                 zTitle = zName,\n",
        "                 Title = Title, \n",
        "                 az=az, \n",
        "                 yMin = yMin, \n",
        "                 yMax = yMax, \n",
        "                 zAsPercent=zAsPercent)\n",
        "  return\n",
        "\n",
        "#Plotting function from a pandas series\n",
        "def plotSerie(data,\n",
        "              Title = 'True Price Surface',\n",
        "              az=320,\n",
        "              yMin = KMin,\n",
        "              yMax = KMax, \n",
        "              zAsPercent = False):\n",
        "  \n",
        "\n",
        "  plotGridCustom(data.index.to_frame().values, \n",
        "                 data.values,\n",
        "                 xTitle = dataSet.index.names[1],\n",
        "                 yTitle = dataSet.index.names[0],\n",
        "                 zTitle = data.name,\n",
        "                 Title = Title, \n",
        "                 az=az, \n",
        "                 yMin = yMin, \n",
        "                 yMax = yMax, \n",
        "                 zAsPercent = zAsPercent)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Za78IddfhsqS",
        "colab": {}
      },
      "source": [
        "plt.get_cmap(\"plasma\")(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wUKTeMfH2IhD",
        "colab": {}
      },
      "source": [
        "plotSurface(dataSet, \"Price\", Title = 'True Price Surface')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YmQpNeyis0iV",
        "colab": {}
      },
      "source": [
        "inverseTransform(scaledDataSet, scaler).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "REWz2Qinm4iZ",
        "colab": {}
      },
      "source": [
        "#Plot predicted value, benchmark value, absoluate error and relative error\n",
        "#It also compute RMSE between predValue and refValue\n",
        "#predValue : approximated value \n",
        "#refValue : benchamrk value\n",
        "#quantityName : name for approximated quantity\n",
        "#az : azimuth i.e. angle of view for surface\n",
        "#yMin : minimum value for y axis\n",
        "#yMax : maximum value for y axis\n",
        "def predictionDiagnosis(predValue, \n",
        "                        refValue, \n",
        "                        quantityName, \n",
        "                        az=320,\n",
        "                        yMin = KMin,\n",
        "                        yMax = KMax):\n",
        "  title = \"Predicted \" + quantityName + \" surface\"\n",
        "  plotSerie(predValue.rename(quantityName), \n",
        "            Title = title, \n",
        "            az=az,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax)\n",
        "  \n",
        "  title = \"True \" + quantityName + \" surface\"\n",
        "  plotSerie(refValue.rename(quantityName), \n",
        "            Title = title, \n",
        "            az=az,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax)\n",
        "  \n",
        "  title = quantityName + \" surface error\"\n",
        "  absoluteError = np.abs(predValue - refValue) \n",
        "  plotSerie(absoluteError.rename(quantityName + \" Absolute Error\"),\n",
        "            Title = title,\n",
        "            az=az,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax)\n",
        "  \n",
        "  title = quantityName + \" surface error\"\n",
        "  relativeError = np.abs(predValue - refValue) / refValue\n",
        "  plotSerie(relativeError.rename(quantityName + \" Relative Error (%)\"),\n",
        "            Title = title,\n",
        "            az=az,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax, \n",
        "            zAsPercent = True)\n",
        "  \n",
        "  print(\"RMSE : \", np.sqrt(np.mean(np.square(absoluteError))) )\n",
        "  \n",
        "  return\n",
        "\n",
        "#Diagnose Price, theta, gamma and local volatility\n",
        "def modelSummary(price, \n",
        "                 volLocale, \n",
        "                 delta_T, \n",
        "                 gamma_K, \n",
        "                 benchDataset,\n",
        "                 sigma=0.3, \n",
        "                 az=40,\n",
        "                 yMin = KMin,\n",
        "                 yMax = KMax):\n",
        "  priceRef = benchDataset[\"Price\"]\n",
        "  predictionDiagnosis(price, \n",
        "                      priceRef, \n",
        "                      \"Price\",\n",
        "                      az=320,\n",
        "                      yMin = yMin,\n",
        "                      yMax = yMax)\n",
        "  \n",
        "  title = \"Price\" + \" scaled surface error\"\n",
        "  absoluteError = np.abs(price - priceRef) / benchDataset[\"VegaRef\"]\n",
        "  plotSerie(absoluteError.rename(\"Price\" + \" Absolute Error normalized by vega 30%\"),\n",
        "            Title = title,\n",
        "            az=320,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax)\n",
        "  \n",
        "  volLocaleRef = benchDataset[\"locvol\"]\n",
        "  predictionDiagnosis(volLocale, \n",
        "                      volLocaleRef, \n",
        "                      \"Local volatility\",\n",
        "                      az=az,\n",
        "                      yMin = yMin,\n",
        "                      yMax = yMax)\n",
        "  \n",
        "  dTRef = benchDataset[\"Theta\"]\n",
        "  predictionDiagnosis(delta_T, \n",
        "                      dTRef, \n",
        "                      \"Theta\",\n",
        "                      az=340,\n",
        "                      yMin = yMin,\n",
        "                      yMax = yMax)\n",
        "  \n",
        "  gKRef = benchDataset[\"Gamma Strike\"]\n",
        "  predictionDiagnosis(gamma_K, \n",
        "                      gKRef, \n",
        "                      \"Gamma Strike\",\n",
        "                      az=340,\n",
        "                      yMin = yMin,\n",
        "                      yMax = yMax)\n",
        "  return\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jQ8OwgHF1I-n"
      },
      "source": [
        "### Implied volatility function calibration by bissection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aL1OsFOh5h--",
        "colab": {}
      },
      "source": [
        "def bs_price(cp, s, k, rf, t, v, div):\n",
        "        \"\"\" Price an option using the Black-Scholes model.\n",
        "        cp: +1/-1 for call/put\n",
        "        s: initial stock price\n",
        "        k: strike price\n",
        "        t: expiration time\n",
        "        v: volatility\n",
        "        rf: risk-free rate\n",
        "        div: dividend\n",
        "        \"\"\"\n",
        "    \n",
        "        d1 = (np.log(s/k)+(rf-div+0.5*v*v)*t)/(v*np.sqrt(t))\n",
        "        d2 = d1 - v*np.sqrt(t)\n",
        "\n",
        "        optprice = (cp*s*np.exp(-div*t)*st.norm.cdf(cp*d1)) - (cp*k*np.exp(-rf*t)*st.norm.cdf(cp*d2))\n",
        "        \n",
        "        return optprice\n",
        "\n",
        "def bissectionMethod(S_0, r, q, implied_vol0, maturity, Strike, refPrice, epsilon):\n",
        "    calibratedSigma = implied_vol0\n",
        "    #Call black-scholes price function for initial value\n",
        "    priceBS = bs_price(-1 ,S0, Strike, r, maturity, calibratedSigma, q)\n",
        "    sigmaUp = 2.0\n",
        "    sigmaInf = epsilon\n",
        "    lossSerie = []\n",
        "    \n",
        "    priceMax = bs_price(-1 ,S0, Strike, r, maturity, sigmaUp, q)\n",
        "    if priceMax < refPrice:\n",
        "        return priceMax, sigmaUp, pd.Series(lossSerie)\n",
        "    \n",
        "    priceMin = bs_price(-1 ,S0, Strike, r, maturity, sigmaInf, q)\n",
        "    if priceMin > refPrice:\n",
        "        return priceMin, sigmaInf, pd.Series(lossSerie) \n",
        "\n",
        "    #Stop the optimization when the error is less than epsilon\n",
        "    while(abs(priceBS - refPrice) > epsilon):\n",
        "        #Update the upper bound or the lower bound \n",
        "        #by comparing calibrated price and the target price \n",
        "        if priceBS < refPrice : \n",
        "            sigmaInf = calibratedSigma\n",
        "        else :\n",
        "            sigmaUp = calibratedSigma\n",
        "        #Update calibratedSigma\n",
        "        calibratedSigma = (sigmaUp + sigmaInf) / 2\n",
        "        #Update calibrated price\n",
        "        priceBS = bs_price(-1 ,S0, Strike, r, maturity, calibratedSigma, q)\n",
        "        #Record the calibration error for this step\n",
        "        lossSerie.append(abs(priceBS - refPrice)) \n",
        "        \n",
        "    return priceBS, calibratedSigma, pd.Series(lossSerie)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qGu1Iz5I1DoF",
        "colab": {}
      },
      "source": [
        "#Execute calibration of implied volatility from estimated price and benchmark price\n",
        "#Then plot esitmated implied vol, absolute and relative error\n",
        "def plotImpliedVol(priceSurface, \n",
        "                   refImpliedVol, \n",
        "                   rIntegralSpline = None, \n",
        "                   qIntegralSpline = None, \n",
        "                   az=40,\n",
        "                   yMin = KMin,\n",
        "                   yMax = KMax,\n",
        "                   relativeErrorVolMax = 10):\n",
        "    df = priceSurface.index.to_frame()\n",
        "    df[\"Price\"] = priceSurface\n",
        "\n",
        "    epsilon = 1e-9\n",
        "    calibrationFunction = lambda x : bissectionMethod(S0, \n",
        "                                                      rIntegralSpline(x[\"Maturity\"])/x[\"Maturity\"] if (rIntegralSpline is not None) else r, \n",
        "                                                      qIntegralSpline(x[\"Maturity\"])/x[\"Maturity\"] if (qIntegralSpline is not None) else q, \n",
        "                                                      0.2, \n",
        "                                                      x[\"Maturity\"], \n",
        "                                                      x[\"Strike\"], \n",
        "                                                      x[\"Price\"], \n",
        "                                                      epsilon)[1]\n",
        "\n",
        "    impliedVol = df.apply(calibrationFunction, axis = 1).rename(\"Implied Volatility\")\n",
        "    impliedVolError = np.abs(impliedVol-refImpliedVol).rename('Absolute Error')\n",
        "    relativeImpliedVolError = (impliedVolError / refImpliedVol).rename(\"Relative error (%)\")\n",
        "    \n",
        "    plotSerie(impliedVol, \n",
        "              Title = 'Implied volatility surface', \n",
        "              az=az,\n",
        "              yMin = yMin,\n",
        "              yMax = yMax)\n",
        "\n",
        "    plotSerie(impliedVolError, \n",
        "              Title = 'Implied volatility error', \n",
        "              az=az,\n",
        "              yMin = yMin,\n",
        "              yMax = yMax)\n",
        "    \n",
        "    plotSerie(relativeImpliedVolError.clip(0,relativeErrorVolMax / 100.0), \n",
        "              Title = 'Implied volatility relative error', \n",
        "              az=az,\n",
        "              yMin = yMin,\n",
        "              yMax = yMax,\n",
        "              zAsPercent = True)\n",
        "  \n",
        "    print(\"Implied volalitity RMSE : \", np.sqrt(np.mean(np.square(impliedVolError))) )\n",
        "\n",
        "    return impliedVol"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0m2TzPY7TL_m",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jl7xMrJRRiVk",
        "colab": {}
      },
      "source": [
        "plotSerie(localVolatility[\"LocalVolatility\"][localVolatility.index.get_level_values(\"Maturity\")>0.01],\n",
        "          Title = 'Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.7*S0,\n",
        "          yMax=1.4*S0, zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MhLrX-8Y1DeM",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UpF5ELRnoS73"
      },
      "source": [
        "## Learning Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rwa7gLppaEdi",
        "colab": {}
      },
      "source": [
        "#Import tensorflow for 1.x version \n",
        "from keras.layers import Dense, Input\n",
        "from keras import Model\n",
        "import keras.backend as K\n",
        "import keras.activations as Act\n",
        "from functools import partial\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vGkkvShljABd",
        "colab": {}
      },
      "source": [
        "#Deactivate warning messages\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KqtRsQxRZ9bT",
        "colab": {}
      },
      "source": [
        "hyperparameters = {}\n",
        "#penalization coefficient\n",
        "hyperparameters[\"lambdaLocVol\"] = 100\n",
        "hyperparameters[\"lambdaSoft\"] = 100 \n",
        "hyperparameters[\"lambdaGamma\"] = 10000\n",
        "\n",
        "#Derivative soft constraints parameters\n",
        "hyperparameters[\"lowerBoundTheta\"] = 0.01\n",
        "hyperparameters[\"lowerBoundGamma\"] = 0.00001\n",
        "\n",
        "#Local variance parameters\n",
        "hyperparameters[\"DupireVarCap\"] = 10\n",
        "hyperparameters[\"DupireVolLowerBound\"] = 0.05\n",
        "hyperparameters[\"DupireVolUpperBound\"] = 0.40\n",
        "\n",
        "#Learning scheduler coefficient\n",
        "hyperparameters[\"LearningRateStart\"] = 0.1\n",
        "hyperparameters[\"Patience\"] = 100\n",
        "hyperparameters[\"batchSize\"] = 50\n",
        "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
        "hyperparameters[\"FixedLearningRate\"] = False\n",
        "\n",
        "#Training parameters\n",
        "hyperparameters[\"nbUnits\"] = 200 #number of units for hidden layers\n",
        "hyperparameters[\"maxEpoch\"] = 10000 #maximum number of epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vNDkFJlYdi7h"
      },
      "source": [
        "### Learning scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-4ZrujaHeD0L",
        "colab": {}
      },
      "source": [
        "#Format result from training step\n",
        "def evalAndFormatResult(price, loss, dataSet):\n",
        "\n",
        "    scaledPredPrice = pd.Series(price.flatten(), index = dataSet.index).rename(\"Price\")\n",
        "    predPrice = inverseTransformColumn(dataSet, scaler, scaledPredPrice)\n",
        "    \n",
        "    return predPrice, pd.Series(loss)\n",
        "\n",
        "#Format result from training step when local volatility is computed\n",
        "def evalAndFormatDupireResult(price, volDupire, theta, gamma, dupireVar, loss, dataSet):\n",
        "    predPrice, lossEpoch = evalAndFormatResult(price, loss, dataSet)\n",
        "\n",
        "    predDupire = pd.Series(volDupire.flatten(), index = dataSet.index).rename(\"Dupire\")\n",
        "    \n",
        "    scaledTheta = pd.Series(theta.flatten(), index = dataSet.index).rename(\"Theta\")\n",
        "    predTheta = inverseTransformColumnGreeks(dataSet, scaler, scaledTheta, \n",
        "                                             \"Price\", \"Maturity\")\n",
        "    \n",
        "    scaledGammaK = pd.Series(gamma.flatten(), index = dataSet.index).rename(\"GammaK\")\n",
        "    predGammaK = inverseTransformColumnGreeks(dataSet, scaler, scaledGammaK, \n",
        "                                              \"Price\", \"ChangedStrike\", order = 2)\n",
        "    \n",
        "    return predPrice, predDupire, predTheta, predGammaK, lossEpoch\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B1iXQCMZRIGV",
        "colab": {}
      },
      "source": [
        "#Penalization for pseudo local volatility\n",
        "def intervalRegularization(localVariance, vegaRef, hyperParameters):\n",
        "  lowerVolBound = hyperParameters[\"DupireVolLowerBound\"]\n",
        "  upperVolBound = hyperParameters[\"DupireVolUpperBound\"]\n",
        "  no_nans = tf.clip_by_value(localVariance, 0, hyperParameters[\"DupireVarCap\"])\n",
        "  reg = tf.nn.relu(tf.square(lowerVolBound) - no_nans) + tf.nn.relu(no_nans - tf.square(upperVolBound))\n",
        "  lambdas = hyperParameters[\"lambdaLocVol\"] / tf.reduce_mean(vegaRef)\n",
        "  return lambdas * tf.reduce_mean(tf.boolean_mask(reg, tf.is_finite(reg)))\n",
        "\n",
        "#Add above regularization to the list of penalization\n",
        "def addDupireRegularisation(priceTensor, tensorList, penalizationList, formattingResultFunction, vegaRef, hyperParameters):\n",
        "    updatedPenalizationList = penalizationList + [intervalRegularization(tensorList[-1], vegaRef, hyperParameters)]\n",
        "    return priceTensor, tensorList, updatedPenalizationList, formattingResultFunction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZUe6Ixrf8Nk-",
        "colab": {}
      },
      "source": [
        "#Mini-batch sampling methods for large datasets\n",
        "def selectMiniBatchWithoutReplacement(dataSet, batch_size):\n",
        "    nbObs = dataSet.shape[0]\n",
        "    idx = np.arange(nbObs) \n",
        "    np.random.shuffle(idx) \n",
        "    nbBatches = int(np.ceil(nbObs/batch_size))\n",
        "    xBatchList = []\n",
        "    lastBatchIndex = 0\n",
        "    for i in range(nbBatches):\n",
        "        firstBatchIndex = i*batch_size\n",
        "        lastBatchIndex = (i+1)*batch_size\n",
        "        xBatchList.append(dataSet.iloc[idx[firstBatchIndex:lastBatchIndex],:])\n",
        "    xBatchList.append(dataSet.iloc[idx[lastBatchIndex:],:])\n",
        "    return xBatchList\n",
        "\n",
        "def selectMiniBatchWithReplacement(dataSet, batch_size):\n",
        "    nbObs = dataSet.shape[0] \n",
        "    nbBatches = int(np.ceil(nbObs/batch_size)) + 1\n",
        "    xBatchList = []\n",
        "    lastBatchIndex = 0\n",
        "    for i in range(nbBatches):\n",
        "        idx = np.random.randint(nbObs, size = batch_size)\n",
        "        xBatchList.append(dataSet.iloc[idx,:])\n",
        "    return xBatchList\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y4oKCKdCL2fI",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZXP0y5kveFPg",
        "colab": {}
      },
      "source": [
        "#Train neural network with a decreasing rule for learning rate\n",
        "#NNFactory :  function creating the architecture\n",
        "#dataSet : training data\n",
        "#activateRegularization : boolean, if true add bound penalization to dupire variance\n",
        "#hyperparameters : dictionnary containing various hyperparameters\n",
        "#modelName : name under which tensorflow model is saved\n",
        "def create_train_model(NNFactory, \n",
        "                       dataSet, \n",
        "                       activateRegularization, \n",
        "                       hyperparameters,\n",
        "                       modelName = \"bestModel\"):\n",
        "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
        "    nbEpoch = hyperparameters[\"maxEpoch\"] \n",
        "    fixedLearningRate = (None if hyperparameters[\"FixedLearningRate\"] else hyperparameters[\"LearningRateStart\"])\n",
        "    patience = hyperparameters[\"Patience\"]\n",
        "    \n",
        "    # Go through num_iters iterations (ignoring mini-batching)\n",
        "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
        "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
        "    learningRateEpoch = 0\n",
        "    finalLearningRate = hyperparameters[\"FinalLearningRate\"]\n",
        "\n",
        "    batch_size = hyperparameters[\"batchSize\"]\n",
        "\n",
        "    start = time.time()\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Strike = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
        "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
        "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    if activateRegularization : #Add pseudo local volatility regularisation\n",
        "        price_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
        "                                                                                                                  Strike,\n",
        "                                                                                                                  Maturity, \n",
        "                                                                                                                  scaleTensor, \n",
        "                                                                                                                  strikeMinTensor, \n",
        "                                                                                                                  vegaRef, \n",
        "                                                                                                                  hyperparameters) ,\n",
        "                                                                                                      vegaRef, \n",
        "                                                                                                      hyperparameters)\n",
        "    else :\n",
        "        price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                        Strike, \n",
        "                                                                                        Maturity, \n",
        "                                                                                        scaleTensor, \n",
        "                                                                                        strikeMinTensor, \n",
        "                                                                                        vegaRef, \n",
        "                                                                                        hyperparameters)\n",
        "\n",
        "    price_pred_tensor_sc= tf.multiply( factorPrice, price_pred_tensor)\n",
        "    TensorList[0] = price_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList) \n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "\n",
        "\n",
        "    # Define a train operation to minimize the loss\n",
        "    lr = learningRate\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = dataSet.shape[0]\n",
        "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
        "\n",
        "    \n",
        "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
        "    loss_serie = []\n",
        "\n",
        "    def createFeedDict(batch):\n",
        "        batchSize = batch.shape[0]\n",
        "        feedDict = {Strike : scaledInput[\"ChangedStrike\"].loc[batch.index].values.reshape(batchSize,1),\n",
        "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
        "                    y : batch[\"Price\"].values.reshape(batchSize,1),\n",
        "                    factorPrice : batch[\"DividendFactor\"].values.reshape(batchSize,1), \n",
        "                    learningRateTensor : learningRate,\n",
        "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
        "        return feedDict\n",
        "\n",
        "    #Learning rate is divided by 10 if no imporvement is observed for training loss after \"patience\" epochs\n",
        "    def updateLearningRate(iterNumber, lr, lrEpoch):\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Constant learning rate, stop training\")\n",
        "            return False, lr, lrEpoch\n",
        "        if learningRate > finalLearningRate :\n",
        "            lr *= 0.1\n",
        "            lrEpoch = iterNumber\n",
        "            saver.restore(sess, modelName)\n",
        "            print(\"Iteration : \", lrEpoch, \"new learning rate : \", lr)\n",
        "        else :\n",
        "          print(\"Last Iteration : \", lrEpoch, \"final learning rate : \", lr)\n",
        "          return False, lr, lrEpoch\n",
        "        return True, lr, lrEpoch\n",
        "    \n",
        "    epochFeedDict = createFeedDict(dataSet)\n",
        "\n",
        "    def evalBestModel():\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
        "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
        "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
        "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
        "        print(\"Best Penalization : \", currentBestPenalizations)\n",
        "        return\n",
        "    \n",
        "    for i in range(nbEpoch):\n",
        "        miniBatchList = [dataSet]\n",
        "        penalizationResult = sess.run(penalizationList, feed_dict=epochFeedDict)\n",
        "        lossResult = sess.run(pointwiseError, feed_dict=epochFeedDict)\n",
        "\n",
        "        #miniBatchList = selectMiniBatchWithoutReplacement(dataSet, batch_size)\n",
        "        for k in range(len(miniBatchList)) :\n",
        "            batchFeedDict = createFeedDict(miniBatchList[k])\n",
        "            sess.run(train, feed_dict=batchFeedDict)\n",
        "        \n",
        "        \n",
        "        loss_serie.append(sess.run(loss, feed_dict=epochFeedDict))\n",
        "\n",
        "        if (len(loss_serie) < 2) or (loss_serie[-1] <= min(loss_serie)):\n",
        "          #Save model as model is improved\n",
        "          saver.save(sess, modelName)\n",
        "        if (np.isnan(loss_serie[-1]) or  #Unstable model\n",
        "            ( (i-learningRateEpoch >= patience) and (min(loss_serie[-patience:]) > min(loss_serie)) ) ) : #No improvement for training loss during the latest 100 iterations\n",
        "          continueTraining, learningRate, learningRateEpoch = updateLearningRate(i, learningRate, learningRateEpoch)\n",
        "          if continueTraining :\n",
        "            evalBestModel()\n",
        "          else :\n",
        "            break\n",
        "    saver.restore(sess, modelName)  \n",
        "    \n",
        "    evalBestModel()\n",
        "\n",
        "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    end = time.time()\n",
        "    print(\"Training Time : \", end - start)\n",
        "    \n",
        "    return formattingFunction(*evalList, loss_serie, dataSet) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vn3rCCVOeCyX",
        "colab": {}
      },
      "source": [
        "#Evaluate neural network without training, it restores parameters obtained from a pretrained model \n",
        "#NNFactory :  function creating the neural architecture\n",
        "#dataSet : dataset on which neural network is evaluated \n",
        "#activateRegularization : boolean, if true add bound penalization for dupire variance\n",
        "#hyperparameters : dictionnary containing various hyperparameters\n",
        "#modelName : name of tensorflow model to restore\n",
        "def create_eval_model(NNFactory, \n",
        "                      dataSet, \n",
        "                      activateRegularization, \n",
        "                      hyperparameters,\n",
        "                      modelName = \"bestModel\"):\n",
        "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
        "    \n",
        "    # Go through num_iters iterations (ignoring mini-batching)\n",
        "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
        "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
        "\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Strike = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
        "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
        "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    if activateRegularization : \n",
        "        price_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
        "                                                                                                                  Strike,\n",
        "                                                                                                                  Maturity, \n",
        "                                                                                                                  scaleTensor, \n",
        "                                                                                                                  strikeMinTensor, \n",
        "                                                                                                                  vegaRef,\n",
        "                                                                                                                  hyperparameters,\n",
        "                                                                                                                  IsTraining=False), \n",
        "                                                                                                      vegaRef,\n",
        "                                                                                                      hyperparameters )\n",
        "    else :\n",
        "        price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                        Strike, \n",
        "                                                                                        Maturity, \n",
        "                                                                                        scaleTensor, \n",
        "                                                                                        strikeMinTensor,\n",
        "                                                                                        vegaRef,\n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        IsTraining=False)\n",
        "\n",
        "    price_pred_tensor_sc= tf.multiply(factorPrice,price_pred_tensor)\n",
        "    TensorList[0] = price_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList)\n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "\n",
        "    # Define a train operation to minimize the loss\n",
        "    lr = learningRate \n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = dataSet.shape[0]\n",
        "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
        "\n",
        "    \n",
        "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
        "    loss_serie = []\n",
        "\n",
        "    def createFeedDict(batch):\n",
        "        batchSize = batch.shape[0]\n",
        "        feedDict = {Strike : scaledInput[\"ChangedStrike\"].loc[batch.index].values.reshape(batchSize,1),\n",
        "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
        "                    y : batch[\"Price\"].values.reshape(batchSize,1),\n",
        "                    factorPrice : batch[\"DividendFactor\"].values.reshape(batchSize,1), \n",
        "                    learningRateTensor : learningRate,\n",
        "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
        "        return feedDict\n",
        "    \n",
        "    epochFeedDict = createFeedDict(dataSet)\n",
        "\n",
        "    def evalBestModel():\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
        "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
        "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
        "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
        "        print(\"Best Penalization : \", currentBestPenalizations)\n",
        "        return\n",
        "    \n",
        "    saver.restore(sess, modelName)  \n",
        "    \n",
        "    evalBestModel()\n",
        "\n",
        "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    \n",
        "    return formattingFunction(*evalList, [0], dataSet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-qgxVPguB_6P"
      },
      "source": [
        "### Convex architecture (Price only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JKjTFU6ioS76",
        "colab": {}
      },
      "source": [
        "\n",
        "#Soft constraints for strike convexity and strike/maturity monotonicity  \n",
        "def arbitragePenalties(priceTensor, strikeTensor, maturityTensor, scaleTensor, vegaRef, hyperparameters):\n",
        "\n",
        "    dK = tf.gradients(priceTensor, strikeTensor, name=\"dK\")\n",
        "    hK = tf.gradients(dK[0], strikeTensor, name=\"hK\") / tf.square(scaleTensor)\n",
        "    theta = tf.gradients(priceTensor,maturityTensor,name=\"dT\")\n",
        "    \n",
        "    lambdas = hyperparameters[\"lambdaSoft\"]  / tf.reduce_mean(vegaRef) \n",
        "    lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "    lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "    grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta[0] + lowerBoundTheta ))\n",
        "    hessian_penalty = lambdas * hyperparameters[\"lowerBoundGamma\"] * tf.reduce_mean(tf.nn.relu(-hK[0] + lowerBoundGamma ))\n",
        "    \n",
        "    return [grad_penalty, hessian_penalty]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njypUjlPLFeK",
        "colab": {}
      },
      "source": [
        "#Tools function for Neural network architecture\n",
        "\n",
        "#Initilize weights as positive\n",
        "def positiveKernelInitializer(shape, \n",
        "                              dtype=None, \n",
        "                              partition_info=None):\n",
        "  return tf.abs(tf.keras.initializers.normal()(shape,dtype=dtype, partition_info=partition_info))\n",
        "\n",
        "#Soft convex layer\n",
        "def convexLayer(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    return tf.nn.softplus(layer)\n",
        "\n",
        "#Soft monotonic layer\n",
        "def monotonicLayer(n_units,  tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor, \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    \n",
        "    return tf.nn.sigmoid(layer)\n",
        "\n",
        "#Soft convex layer followed by output layer for regression \n",
        "def convexOutputLayer(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=2*n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
        "                            activation = 'softplus')\n",
        "    \n",
        "     \n",
        "    layer = tf.layers.dense(layer, \n",
        "                            units=1,\n",
        "                            kernel_initializer=positiveKernelInitializer,\n",
        "                            activation = 'softplus')\n",
        "    \n",
        "    return layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Neural network factory for Hybrid approach : splitted network with soft contraints\n",
        "def NNArchitectureConstrained(n_units, \n",
        "                              strikeTensor,\n",
        "                              maturityTensor, \n",
        "                              scaleTensor, \n",
        "                              strikeMinTensor, \n",
        "                              vegaRef, \n",
        "                              hyperparameters,\n",
        "                              IsTraining=True):\n",
        "  #First splitted layer\n",
        "  hidden1S = convexLayer(n_units = n_units,\n",
        "                         tensor = strikeTensor,\n",
        "                         isTraining=IsTraining, \n",
        "                         name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M = monotonicLayer(n_units = n_units,\n",
        "                            tensor = maturityTensor, \n",
        "                            isTraining = IsTraining, \n",
        "                            name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
        "  \n",
        "  #Second and output layer\n",
        "  out = convexOutputLayer(n_units = n_units,\n",
        "                          tensor = hidden1,\n",
        "                          isTraining = IsTraining,\n",
        "                          name = \"Output\")\n",
        "  #Soft constraints\n",
        "  penaltyList = arbitragePenalties(out, strikeTensor, \n",
        "                                   maturityTensor, \n",
        "                                   scaleTensor, \n",
        "                                   vegaRef, \n",
        "                                   hyperparameters)\n",
        "  \n",
        "  return out, [out], penaltyList, evalAndFormatResult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6i9lEOpGoS79",
        "colab": {}
      },
      "source": [
        "plt.plot(dataSet.index.get_level_values(\"Strike\"), dataSet[\"Price\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uT7gq2LRSXTI",
        "colab": {}
      },
      "source": [
        "y_pred0, lossSerie0 = create_train_model(NNArchitectureConstrained, scaledDataSet, False, hyperparameters, modelName = \"softConvexHybridModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-fv_bsyPSXTK",
        "colab": {}
      },
      "source": [
        "print(\"Minimum error : \",lossSerie0.min())\n",
        "plotEpochLoss(lossSerie0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kwi7u3dELnws",
        "colab": {}
      },
      "source": [
        "lossSerie0.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MUEXO111oS8G",
        "colab": {}
      },
      "source": [
        "y_pred0, lossSerie0 = create_eval_model(NNArchitectureConstrained, \n",
        "                                        scaledDataSet, \n",
        "                                        False, \n",
        "                                        hyperparameters, \n",
        "                                        modelName = \"softConvexHybridModel\")\n",
        "predictionDiagnosis(y_pred0, dataSet[\"Price\"], \" Price \")\n",
        "impV0 = plotImpliedVol(y_pred0, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L-mNSLXHhRfm",
        "colab": {}
      },
      "source": [
        "y_pred0.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2STQwalmhoyR",
        "colab": {}
      },
      "source": [
        "y_pred0.loc[(midS0,slice(None))].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V9tKlu5Ji1YF",
        "colab": {}
      },
      "source": [
        "dataSet[\"Price\"].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oRI-8R9ezLLa",
        "colab": {}
      },
      "source": [
        "y_pred0Test, lossSerie0Test = create_eval_model(NNArchitectureConstrained, \n",
        "                                                scaledDataSetTest, \n",
        "                                                False, \n",
        "                                                hyperparameters, \n",
        "                                                modelName = \"softConvexHybridModel\")\n",
        "predictionDiagnosis(y_pred0Test, dataSetTest[\"Price\"], \" Price \")\n",
        "impV0Test = plotImpliedVol(y_pred0Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nes-RItOzLIM",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bQT4PIYWzK81",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KWdzg9DEv7eH"
      },
      "source": [
        "### Unconstrained neural network (Price only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HnG0q3QisrMt",
        "colab": {}
      },
      "source": [
        "#Unconstrained dense layer\n",
        "def unconstrainedLayer(n_units,  tensor, isTraining, name, activation = K.softplus):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor, \n",
        "                            units=n_units,\n",
        "                            activation=activation,  \n",
        "                            kernel_initializer=tf.keras.initializers.he_normal())\n",
        "    return layer\n",
        "\n",
        "#Factory for unconstrained network\n",
        "def NNArchitectureUnconstrained(n_units, \n",
        "                                strikeTensor,\n",
        "                                maturityTensor, \n",
        "                                scaleTensor, \n",
        "                                strikeMinTensor, \n",
        "                                vegaRef,\n",
        "                                hyperparameters,\n",
        "                                IsTraining=True):\n",
        "  \n",
        "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
        "  \n",
        "  #First layer\n",
        "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = inputLayer,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden1\")\n",
        "  \n",
        "  #Second layer\n",
        "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden1,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden2\")\n",
        "  #Output layer \n",
        "  out = unconstrainedLayer(n_units = 1,\n",
        "                           tensor = hidden2,\n",
        "                           isTraining=IsTraining, \n",
        "                           name = \"Output\",\n",
        "                           activation = None)\n",
        "  \n",
        "  return out, [out], [], evalAndFormatResult\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YZGyKcpsrMv",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WdWudyC2srMw",
        "colab": {}
      },
      "source": [
        "y_pred1, lossSerie1 = create_train_model(NNArchitectureUnconstrained, \n",
        "                                         scaledDataSet, \n",
        "                                         False, \n",
        "                                         hyperparameters,\n",
        "                                         modelName = \"unconstrainedModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AzjHIzRUsrMx",
        "colab": {}
      },
      "source": [
        "print(\"Minimum error : \",lossSerie1.min())\n",
        "plotEpochLoss(lossSerie1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TArgJP5fsrMy",
        "colab": {}
      },
      "source": [
        "lossSerie1.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IzlURNdisrM0",
        "colab": {}
      },
      "source": [
        "y_pred1, lossSerie1 = create_eval_model(NNArchitectureUnconstrained, \n",
        "                                        scaledDataSet, \n",
        "                                        False, \n",
        "                                        hyperparameters,\n",
        "                                        modelName = \"unconstrainedModel\")\n",
        "predictionDiagnosis(y_pred1, dataSet[\"Price\"], \" Price \")\n",
        "impV1 = plotImpliedVol(y_pred1, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5fP6NJ4srM1",
        "colab": {}
      },
      "source": [
        "y_pred1.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_pMZlMbdsrM5",
        "colab": {}
      },
      "source": [
        "y_pred1.loc[(midS0,slice(None))].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lU1LIqnPsrM9",
        "colab": {}
      },
      "source": [
        "dataSet[\"Price\"].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bPrMgqTtsrNB",
        "colab": {}
      },
      "source": [
        "y_pred1Test, lossSerie1Test = create_eval_model(NNArchitectureUnconstrained, \n",
        "                                                scaledDataSetTest, \n",
        "                                                False, \n",
        "                                                hyperparameters,\n",
        "                                                modelName = \"unconstrainedModel\")\n",
        "predictionDiagnosis(y_pred1Test, dataSetTest[\"Price\"], \" Price \")\n",
        "impV1Test = plotImpliedVol(y_pred1Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ONoMCWOCXWO-"
      },
      "source": [
        "## Dupire formula implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QWZzojhoDJRn",
        "colab": {}
      },
      "source": [
        "#Dupire formula from exact derivative computation\n",
        "def dupireFormula(HessianStrike, \n",
        "                  GradMaturity, \n",
        "                  Strike,\n",
        "                  scaleTensor,\n",
        "                  strikeMinTensor,\n",
        "                  IsTraining=True):\n",
        "  twoConstant = tf.constant(2.0)\n",
        "  dupireVar = tf.math.divide(tf.math.divide(tf.math.scalar_mul(twoConstant,GradMaturity), \n",
        "                                            HessianStrike), \n",
        "                             tf.square(Strike + strikeMinTensor / scaleTensor))\n",
        "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
        "  dupireVolTensor = tf.sqrt(dupireVar) \n",
        "  return dupireVolTensor, dupireVar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o59zKQXO8LKU",
        "colab": {}
      },
      "source": [
        "#Dupire formula with derivative obtained from native tensorflow algorithmic differentiation\n",
        "def rawDupireFormula(priceTensor, \n",
        "                     adjustedStrikeTensor, \n",
        "                     maturityTensor,\n",
        "                     scaleTensor,\n",
        "                     strikeMinTensor,\n",
        "                     IsTraining=True):\n",
        "  batchSize = tf.shape(adjustedStrikeTensor)[0]\n",
        "  dK = tf.reshape(tf.gradients(priceTensor, adjustedStrikeTensor, name=\"dK\")[0], shape=[batchSize,-1])\n",
        "  hK = tf.reshape(tf.gradients(dK, adjustedStrikeTensor, name=\"hK\")[0], shape=[batchSize,-1])\n",
        "  dupireDenominator = tf.square(adjustedStrikeTensor + strikeMinTensor / scaleTensor) * hK\n",
        "\n",
        "  dT = tf.reshape(tf.gradients(priceTensor,maturityTensor,name=\"dT\")[0], shape=[batchSize,-1])\n",
        "\n",
        "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
        "  dupireVar = 2 * dT / dupireDenominator\n",
        "  dupireVol = tf.sqrt(dupireVar) \n",
        "  return  dupireVol, dT, hK / tf.square(scaleTensor), dupireVar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B3YTL72Tw_FV",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s192l-r-B6lD"
      },
      "source": [
        "### Hybrid architecture (Exact derivatives)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1mtTLFrQC36u",
        "colab": {}
      },
      "source": [
        "def exact_derivatives(Strike, Maturity):\n",
        "    w1K = tf.get_default_graph().get_tensor_by_name( 'dense/kernel:0')\n",
        "    w1T = tf.get_default_graph().get_tensor_by_name( 'dense_1/kernel:0')\n",
        "    w2 = tf.get_default_graph().get_tensor_by_name( 'dense_2/kernel:0')\n",
        "    w3 = tf.get_default_graph().get_tensor_by_name( 'dense_3/kernel:0')\n",
        "\n",
        "    b1K = tf.get_default_graph().get_tensor_by_name( 'dense/bias:0')\n",
        "    b1T = tf.get_default_graph().get_tensor_by_name( 'dense_1/bias:0')\n",
        "    b2 = tf.get_default_graph().get_tensor_by_name( 'dense_2/bias:0')\n",
        "    b3 = tf.get_default_graph().get_tensor_by_name( 'dense_3/bias:0')\n",
        "\n",
        "    Z1K= tf.nn.softplus(tf.matmul(Strike, w1K) + b1K)\n",
        "    Z1T= tf.nn.sigmoid(tf.matmul(Maturity, w1T) + b1T)\n",
        "\n",
        "    Z= tf.concat([Z1K, Z1T], axis=-1)\n",
        "    I2=tf.matmul(Z, w2) + b2\n",
        "    Z2=tf.nn.softplus(I2)\n",
        "    I3=tf.matmul(Z2, w3) + b3\n",
        "    F=tf.nn.softplus(I3)\n",
        "\n",
        "    D1K= tf.nn.sigmoid(tf.matmul(Strike, w1K) + b1K)\n",
        "    I2K=tf.multiply(D1K, w1K)\n",
        "    Z2K = tf.concat([I2K, tf.scalar_mul(tf.constant(0.0),I2K)],axis=-1)\n",
        "   \n",
        "    dI2dK=tf.matmul(Z2K, w2)\n",
        "    Z2w3=tf.multiply(tf.nn.sigmoid(I2),dI2dK)\n",
        "    dI3dK=tf.matmul(Z2w3, w3)\n",
        "    dF_dK=tf.multiply(tf.nn.sigmoid(I3),dI3dK)\n",
        "    \n",
        "    D1T= sigmoidGradient(tf.matmul(Maturity,w1T) + b1T)\n",
        "    I2T=tf.multiply(D1T, w1T)\n",
        "    Z2T = tf.concat([tf.scalar_mul(tf.constant(0.0),I2T), I2T],axis=-1)\n",
        "   \n",
        "    dI2dT=tf.matmul(Z2T, w2)\n",
        "    Z2w3=tf.multiply(tf.nn.sigmoid(I2),dI2dT)\n",
        "    dI3dT=tf.matmul(Z2w3, w3)\n",
        "    dF_dT=tf.multiply(tf.nn.sigmoid(I3),dI3dT)\n",
        "    \n",
        "    \n",
        "    d2F_dK2=tf.multiply(sigmoidGradient(I3),tf.square(dI3dK))\n",
        "    DD1K=sigmoidGradient(tf.matmul(Strike, w1K) + b1K)\n",
        "    w1K2=tf.multiply(w1K,w1K)\n",
        "    ID2K=tf.multiply(DD1K,w1K2)\n",
        "    ZD2K = tf.concat([ID2K, tf.scalar_mul(tf.constant(0.0),ID2K)],axis=-1)\n",
        "   \n",
        "    d2I2_dK2=tf.matmul(ZD2K, w2)\n",
        "    \n",
        "    ZD2=tf.multiply(sigmoidGradient(I2), tf.square(dI2dK)) \n",
        "    ZD2+=tf.multiply(tf.nn.sigmoid(I2),d2I2_dK2)\n",
        "    d2I3dK2=tf.matmul(ZD2, w3)\n",
        "    \n",
        "    d2F_dK2+=tf.multiply(tf.nn.sigmoid(I3),d2I3dK2)\n",
        "    \n",
        "    return dF_dT, dF_dK, d2F_dK2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_fkoZMVXWO-",
        "colab": {}
      },
      "source": [
        "#Tools functions for neural architecture\n",
        "def positiveKernelInitializer(shape, \n",
        "                              dtype=None, \n",
        "                              partition_info=None):\n",
        "  return tf.abs(tf.keras.initializers.normal()(shape,dtype=dtype, partition_info=partition_info))\n",
        "\n",
        "\n",
        "#Neural network architecture\n",
        "def convexLayer1(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    return tf.nn.softplus(layer), layer\n",
        "\n",
        "def monotonicLayer1(n_units,  tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor, \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    \n",
        "    return tf.nn.sigmoid(layer),layer\n",
        "\n",
        "def convexOutputLayer1(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=2*n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
        "                            activation = 'softplus') \n",
        "    \n",
        "     \n",
        "    layer = tf.layers.dense(layer, \n",
        "                            units=1,\n",
        "                            kernel_initializer=positiveKernelInitializer, \n",
        "                            activation = 'softplus')\n",
        "    \n",
        "    return layer, layer \n",
        "  \n",
        "\n",
        "def convexLayerHybrid1(n_units, \n",
        "                      tensor, \n",
        "                      isTraining, \n",
        "                      name, \n",
        "                      activationFunction2 = Act.softplus,\n",
        "                      activationFunction1 = Act.exponential,\n",
        "                      isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=positiveKernelInitializer)\n",
        "    l1,l2 = tf.split(layer,2,1)\n",
        "    output = tf.concat([activationFunction1(l1),activationFunction2(l2)],axis=-1)\n",
        "    return output , layer\n",
        "\n",
        "def sigmoidGradient(inputTensor):\n",
        "  return tf.nn.sigmoid(inputTensor) * ( 1 - tf.nn.sigmoid(inputTensor) )\n",
        "\n",
        "def sigmoidHessian(inputTensor) :\n",
        "  return (tf.square(1 - tf.nn.sigmoid(inputTensor)) -\n",
        "          tf.nn.sigmoid(inputTensor) * (1 - tf.nn.sigmoid(inputTensor)))\n",
        "\n",
        "  \n",
        "def NNArchitectureConstrainedDupire(n_units, \n",
        "                                    strikeTensor,\n",
        "                                    maturityTensor, \n",
        "                                    scaleTensor, \n",
        "                                    strikeMinTensor,\n",
        "                                    vegaRef, \n",
        "                                    hyperparameters,\n",
        "                                    IsTraining=True):\n",
        "  #First splitted layer\n",
        "  hidden1S, layer1S = convexLayer1(n_units = n_units,\n",
        "                                   tensor = strikeTensor,\n",
        "                                   isTraining=IsTraining,\n",
        "                                   name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M,layer1M = monotonicLayer1(n_units = n_units,\n",
        "                                     tensor = maturityTensor,\n",
        "                                     isTraining = IsTraining,\n",
        "                                     name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
        "  \n",
        "  #Second layer and output layer\n",
        "  out, layer = convexOutputLayer1(n_units = n_units,\n",
        "                                  tensor = hidden1,\n",
        "                                  isTraining = IsTraining,\n",
        "                                  name = \"Output\")\n",
        "  \n",
        "  \n",
        "  dT, dS, HS = exact_derivatives(strikeTensor, maturityTensor)\n",
        "  \n",
        "  \n",
        "  \n",
        "  #Local volatility\n",
        "  dupireVol, dupireVar = dupireFormula(HS, dT, \n",
        "                                       strikeTensor,\n",
        "                                       scaleTensor,\n",
        "                                       strikeMinTensor, \n",
        "                                       IsTraining=IsTraining)\n",
        "  \n",
        "  #Soft constraints on price\n",
        "  lambdas = hyperparameters[\"lambdaSoft\"]\n",
        "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-dT + lowerBoundTheta) / vegaRef)\n",
        "  HSScaled = HS / tf.square(scaleTensor)\n",
        "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(- HSScaled + lowerBoundGamma) / vegaRef)\n",
        "  \n",
        "  return out, [out, dupireVol, dT, HSScaled, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SHfggURehdvS",
        "colab": {}
      },
      "source": [
        "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRIe-lSD8A0H",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8NX751W-BNad",
        "colab": {}
      },
      "source": [
        "y_pred2, volLocale2, dNN_T2, gNN_K2, lossSerie2 = create_train_model(NNArchitectureConstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"convexHybridMatthewDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XnX-RKpgBO0y",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "puVhFTMTBcQ5",
        "colab": {}
      },
      "source": [
        "lossSerie2.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x5k1RAfyearf",
        "colab": {}
      },
      "source": [
        "y_pred2, volLocale2, dNN_T2, gNN_K2, lossSerie2 = create_eval_model(NNArchitectureConstrainedDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    False, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"convexHybridMatthewDupireVolModel\")\n",
        "modelSummary(y_pred2, volLocale2, dNN_T2, gNN_K2, dataSet)\n",
        "impV2 = plotImpliedVol(y_pred2, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xj1pPdXLZHrk",
        "colab": {}
      },
      "source": [
        "volLocale2.loc[(midS0,slice(None))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R0jZy5U7JqVt",
        "colab": {}
      },
      "source": [
        "y_pred2Test, volLocale2Test, dNN_T2Test, gNN_K2Test, lossSerie2Test = create_eval_model(NNArchitectureConstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexHybridMatthewDupireVolModel\")\n",
        "modelSummary(y_pred2Test, volLocale2Test, dNN_T2Test, gNN_K2Test, dataSetTest)\n",
        "impV2Test = plotImpliedVol(y_pred2Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "de9dKmAAHI2m",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hUMtwNuguMRu"
      },
      "source": [
        "### Hybrid Network (Derivatives from algorithmic differentiation) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YOiA45eZJqSb",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j8m0kMz_udFn",
        "colab": {}
      },
      "source": [
        "\n",
        "def NNArchitectureConstrainedRawDupire(n_units, \n",
        "                                       strikeTensor,\n",
        "                                       maturityTensor,\n",
        "                                       scaleTensor,\n",
        "                                       strikeMinTensor, \n",
        "                                       vegaRef, \n",
        "                                       hyperparameters,\n",
        "                                       IsTraining=True):\n",
        "  #First splitted layer\n",
        "  hidden1S = convexLayer(n_units = n_units,\n",
        "                         tensor = strikeTensor,\n",
        "                         isTraining=IsTraining, \n",
        "                         name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M = monotonicLayer(n_units = n_units,\n",
        "                            tensor = maturityTensor, \n",
        "                            isTraining = IsTraining, \n",
        "                            name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
        "  \n",
        "  #Second hidden layer and output layer\n",
        "  out = convexOutputLayer(n_units = n_units,\n",
        "                          tensor = hidden1,\n",
        "                          isTraining = IsTraining,\n",
        "                          name = \"Output\")\n",
        "  \n",
        "  #Compute local volatility\n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor, \n",
        "                                                     maturityTensor, \n",
        "                                                     scaleTensor, \n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "\n",
        "  #Soft constraints for no-arbitrage\n",
        "  lambdas = hyperparameters[\"lambdaSoft\"] \n",
        "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta + lowerBoundTheta) / vegaRef)\n",
        "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(-hK + lowerBoundGamma) / vegaRef)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GntRmtJkudCz",
        "colab": {}
      },
      "source": [
        "y_pred3, volLocale3, dNN_T3, gNN_K3, lossSerie3 = create_train_model(NNArchitectureConstrainedRawDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"convexHybridDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2lkQVBdnuc_-",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bfEKwrHCuc9B",
        "colab": {}
      },
      "source": [
        "lossSerie3.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MA24IOZoucqj",
        "colab": {}
      },
      "source": [
        "y_pred3, volLocale3, dNN_T3, gNN_K3, lossSerie3 = create_eval_model(NNArchitectureConstrainedRawDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    False,\n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"convexHybridDupireVolModel\")\n",
        "modelSummary(y_pred3, volLocale3, dNN_T3, gNN_K3, dataSet)\n",
        "impV3 = plotImpliedVol(y_pred3, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R5BCRxbMuc0p",
        "colab": {}
      },
      "source": [
        "volLocale3.loc[(midS0,slice(None))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bS4fYI7ucjO",
        "colab": {}
      },
      "source": [
        "y_pred3Test, volLocale3Test, dNN_T3Test, gNN_K3Test, lossSerie3Test = create_eval_model(NNArchitectureConstrainedRawDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexHybridDupireVolModel\")\n",
        "modelSummary(y_pred3Test, volLocale3Test, dNN_T3Test, gNN_K3Test, dataSetTest)\n",
        "impV3Test = plotImpliedVol(y_pred3Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y02WGdGVucZ5",
        "colab": {}
      },
      "source": [
        "dNN_T3Test[dNN_T3Test<=0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iRDJhGWoV2mE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r9EvOdg0kU2P"
      },
      "source": [
        "### Standard network with soft constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1UXinUrkUm_",
        "colab": {}
      },
      "source": [
        "def NNArchitectureVanillaSoftDupire(n_units, strikeTensor,\n",
        "                                    maturityTensor,\n",
        "                                    scaleTensor,\n",
        "                                    strikeMinTensor,\n",
        "                                    vegaRef,\n",
        "                                    hyperparameters,\n",
        "                                    IsTraining=True):\n",
        "  \n",
        "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
        "  #First layer\n",
        "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = inputLayer,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden1\")\n",
        "  #Second layer\n",
        "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden1,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden2\")\n",
        "  #Output layer\n",
        "  out = unconstrainedLayer(n_units = 1,\n",
        "                           tensor = hidden2,\n",
        "                           isTraining=IsTraining, \n",
        "                           name = \"Output\",\n",
        "                           activation = None)\n",
        "  #Local volatility \n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
        "                                                     maturityTensor,\n",
        "                                                     scaleTensor,\n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "  #Soft constraints for no arbitrage\n",
        "  lambdas = hyperparameters[\"lambdaSoft\"] \n",
        "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta + lowerBoundTheta) / vegaRef)\n",
        "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(-hK + lowerBoundGamma) / vegaRef)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k-FDViM6keCA",
        "colab": {}
      },
      "source": [
        "y_pred4, volLocale4, dNN_T4, gNN_K4, lossSerie4 = create_train_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"convexSoftDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqrPGUcHkd_a",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PBoUrx83kd9W",
        "colab": {}
      },
      "source": [
        "lossSerie4.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI7jsuqPYvI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred4, volLocale4, dNN_T4, gNN_K4, lossSerie4 = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                    scaledDataSet, \n",
        "                                                                    False, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"convexSoftDupireVolModel\")\n",
        "modelSummary(y_pred4, volLocale4, dNN_T4, gNN_K4, dataSet)\n",
        "impV4 = plotImpliedVol(y_pred4, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rwpPXhszkd6Z",
        "colab": {}
      },
      "source": [
        "volLocale4.loc[(midS0,slice(None))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAfSOpREkdzc",
        "colab": {}
      },
      "source": [
        "y_pred4Test, volLocale4Test, dNN_T4Test, gNN_K4Test, lossSerie4Test = create_eval_model(NNArchitectureVanillaSoftDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexSoftDupireVolModel\")\n",
        "modelSummary(y_pred4Test, volLocale4Test, dNN_T4Test, gNN_K4Test, dataSetTest)\n",
        "impV4Test = plotImpliedVol(y_pred4Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OCZXZWCIkdxR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0SsMFMnokdu-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9LHmCnZikdr1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Atqa9I8Gkdn-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_eJG7_513hNC"
      },
      "source": [
        "### Unconstrained standard network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6skBo-i-JqOx",
        "colab": {}
      },
      "source": [
        "def NNArchitectureUnconstrainedDupire(n_units, strikeTensor,\n",
        "                                      maturityTensor,\n",
        "                                      scaleTensor,\n",
        "                                      strikeMinTensor, \n",
        "                                      vegaRef,\n",
        "                                      hyperparameters,\n",
        "                                      IsTraining=True):\n",
        "  \n",
        "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
        "  \n",
        "  #First layer\n",
        "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = inputLayer,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden1\")\n",
        "  #Second layer\n",
        "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden1,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden2\")\n",
        "  #Ouput layer\n",
        "  out = unconstrainedLayer(n_units = 1,\n",
        "                           tensor = hidden2,\n",
        "                           isTraining=IsTraining, \n",
        "                           name = \"Output\",\n",
        "                           activation = None)\n",
        "  #Local volatility\n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
        "                                                     maturityTensor,\n",
        "                                                     scaleTensor,\n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [], evalAndFormatDupireResult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cIEIgBTp3mFn",
        "colab": {}
      },
      "source": [
        "y_pred5, volLocale5, dNN_T5, gNN_K5, lossSerie5 = create_train_model(NNArchitectureUnconstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"unconstrainedDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "evKBtgfB3mBj",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X8qTeMJm3l-i",
        "colab": {}
      },
      "source": [
        "lossSerie5.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odwXtVHNYvJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred5, volLocale5, dNN_T5, gNN_K5, lossSerie5 = create_eval_model(NNArchitectureUnconstrainedDupire,\n",
        "                                                                    scaledDataSet,\n",
        "                                                                    False,\n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"unconstrainedDupireVolModel\")\n",
        "modelSummary(y_pred5, volLocale5, dNN_T5, gNN_K5, dataSet)\n",
        "impV5 = plotImpliedVol(y_pred5, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a306_WrY3l7R",
        "colab": {}
      },
      "source": [
        "volLocale5.loc[(midS0,slice(None))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vsHhlAW3lpu",
        "colab": {}
      },
      "source": [
        "y_pred5Test, volLocale5Test, dNN_T5Test, gNN_K5Test, lossSerie5Test = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"unconstrainedDupireVolModel\")\n",
        "modelSummary(y_pred5Test, volLocale5Test, dNN_T5Test, gNN_K5Test, dataSetTest)\n",
        "impV5Test = plotImpliedVol(y_pred5Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cUB3Y62YJpwo",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MrvDySFQWK-D"
      },
      "source": [
        "### Hard constrained architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1R2iPlotWQoH",
        "colab": {}
      },
      "source": [
        "#Tools functions for hard constrained neural architecture\n",
        "\n",
        "def convexLayerHard(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    return tf.nn.softplus(layer), layer \n",
        "\n",
        "def monotonicLayerHard(n_units,  tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor, \n",
        "                            units=n_units,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    \n",
        "    return tf.nn.sigmoid(layer),layer\n",
        "\n",
        "def convexOutputLayerHard(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=2*n_units,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
        "                            activation = 'softplus') \n",
        "    \n",
        "     \n",
        "    layer = tf.layers.dense(layer, \n",
        "                            units=1,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=positiveKernelInitializer, \n",
        "                            activation = 'softplus')\n",
        "    \n",
        "    return layer, layer \n",
        "  \n",
        "\n",
        "def convexLayerHybridHard(n_units,\n",
        "                          tensor,\n",
        "                          isTraining,\n",
        "                          name,\n",
        "                          activationFunction2 = Act.softplus,\n",
        "                          activationFunction1 = Act.exponential,\n",
        "                          isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=positiveKernelInitializer)\n",
        "    l1,l2 = tf.split(layer,2,1)\n",
        "    output = tf.concat([activationFunction1(l1),activationFunction2(l2)],axis=-1)\n",
        "    return output , layer\n",
        "\n",
        "def sigmoidGradientHard(inputTensor):\n",
        "  return tf.nn.sigmoid(inputTensor) * ( 1 - tf.nn.sigmoid(inputTensor) )\n",
        "\n",
        "def sigmoidHessianHard(inputTensor) :\n",
        "  return (tf.square(1 - tf.nn.sigmoid(inputTensor)) -\n",
        "          tf.nn.sigmoid(inputTensor) * (1 - tf.nn.sigmoid(inputTensor)))\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "def NNArchitectureHardConstrainedDupire(n_units, strikeTensor, \n",
        "                                        maturityTensor,\n",
        "                                        scaleTensor,\n",
        "                                        strikeMinTensor, \n",
        "                                        vegaRef,\n",
        "                                        hyperparameters,\n",
        "                                        IsTraining=True):\n",
        "  #First layer\n",
        "  hidden1S, layer1S = convexLayerHard(n_units = n_units,\n",
        "                                      tensor = strikeTensor,\n",
        "                                      isTraining=IsTraining,\n",
        "                                      name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M,layer1M = monotonicLayerHard(n_units = n_units,\n",
        "                                        tensor = maturityTensor,\n",
        "                                        isTraining = IsTraining,\n",
        "                                        name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
        "  \n",
        "  #Second layer and output layer\n",
        "  out, layer = convexOutputLayerHard(n_units = n_units,\n",
        "                                     tensor = hidden1,\n",
        "                                     isTraining = IsTraining,\n",
        "                                     name = \"Output\")\n",
        "  #Local volatility\n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
        "                                                     maturityTensor,\n",
        "                                                     scaleTensor,\n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [], evalAndFormatDupireResult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ruwRhj_-WQlh",
        "colab": {}
      },
      "source": [
        "y_pred6, volLocale6, dNN_T6, gNN_K6, lossSerie6 = create_train_model(NNArchitectureHardConstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"convexHardDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W0GD3xYRWQif",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XF5Iq9ZYWQf6",
        "colab": {}
      },
      "source": [
        "lossSerie6.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRpdkOKMYvJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred6, volLocale6, dNN_T6, gNN_K6, lossSerie6 = create_eval_model(NNArchitectureHardConstrainedDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    False, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"convexHardDupireVolModel\")\n",
        "modelSummary(y_pred6, volLocale6, dNN_T6, gNN_K6, dataSet)\n",
        "impV6 = plotImpliedVol(y_pred6, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-oZmufhXWQdk",
        "colab": {}
      },
      "source": [
        "volLocale6.loc[(midS0,slice(None))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_NKwHrxdWQWG",
        "colab": {}
      },
      "source": [
        "y_pred6Test, volLocale6Test, dNN_T6Test, gNN_K6Test, lossSerie6Test = create_eval_model(NNArchitectureHardConstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexHardDupireVolModel\")\n",
        "modelSummary(y_pred6Test, volLocale6Test, dNN_T6Test, gNN_K6Test, dataSetTest)\n",
        "impV6Test = plotImpliedVol(y_pred6Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8W6gaoaKWQTS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n_iWhfZjZJt7"
      },
      "source": [
        "## Dupire regularization \n",
        "\n",
        "Same lines as above except that dupire regularization is now activated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NZBPdHPuB0CX"
      },
      "source": [
        "### Hybrid architecture (Exact derivatives)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmbg_HciZNaW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CMQDARTbZTM8",
        "colab": {}
      },
      "source": [
        "y_pred8, volLocale8, dNN_T8, gNN_K8, lossSerie8 = create_train_model(NNArchitectureConstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     True, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"regularizedConvexHybridMatthewDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "phELMC43ZS-4",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elgx3ip3ZS24",
        "colab": {}
      },
      "source": [
        "lossSerie8.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5kIlaICuZSzV",
        "colab": {}
      },
      "source": [
        "y_pred8, volLocale8, dNN_T8, gNN_K8, lossSerie8 = create_eval_model(NNArchitectureConstrainedDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    True, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"regularizedConvexHybridMatthewDupireVolModel\")\n",
        "modelSummary(y_pred8, volLocale8, dNN_T8, gNN_K8, dataSet)\n",
        "impV8 = plotImpliedVol(y_pred8, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njoK6xVa1ReM",
        "colab": {}
      },
      "source": [
        "y_pred8Test, volLocale8Test, dNN_T8Test, gNN_K8Test, lossSerie8Test = create_eval_model(NNArchitectureConstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        True, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"regularizedConvexHybridMatthewDupireVolModel\")\n",
        "modelSummary(y_pred8Test, volLocale8Test, dNN_T8Test, gNN_K8Test, dataSetTest)\n",
        "impV8Test = plotImpliedVol(y_pred8Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NB3_-zutBt2y"
      },
      "source": [
        "### Unconstrained standard network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xYrYMhOZZ3Pd",
        "colab": {}
      },
      "source": [
        "y_pred9, volLocale9, dNN_T9, gNN_K9, lossSerie9 = create_train_model(NNArchitectureUnconstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     True, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"regularizedUnconstrainedDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wHi2doJQBbZO",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "76G0tLeSBdeD",
        "colab": {}
      },
      "source": [
        "lossSerie9.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AYzkOXUABdXV",
        "colab": {}
      },
      "source": [
        "y_pred9, volLocale9, dNN_T9, gNN_K9, lossSerie9 = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    True, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"regularizedUnconstrainedDupireVolModel\")\n",
        "modelSummary(y_pred9, volLocale9, dNN_T9, gNN_K9, dataSet)\n",
        "impV9 = plotImpliedVol(y_pred9, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t9Zvk6DAbAUn",
        "colab": {}
      },
      "source": [
        "y_pred9Test, volLocale9Test, dNN_T9Test, gNN_K9Test, lossSerie9Test = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        True, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"regularizedUnconstrainedDupireVolModel\")\n",
        "modelSummary(y_pred9Test, volLocale9Test, dNN_T9Test, gNN_K9Test, dataSetTest)\n",
        "impV9Test = plotImpliedVol(y_pred9Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "--dUCOKu4c1v",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5yP7BP6avj5q"
      },
      "source": [
        "### Hybrid Network (Derivatives from algorithmic differentiation) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9u0zPfA6vpA2",
        "colab": {}
      },
      "source": [
        "y_pred10, volLocale10, dNN_T10, gNN_K10, lossSerie10 = create_train_model(NNArchitectureConstrainedRawDupire,\n",
        "                                                                          scaledDataSet,\n",
        "                                                                          True,\n",
        "                                                                          hyperparameters,\n",
        "                                                                          modelName = \"regularizedConvexHybridDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9t_BmpjTvpnU",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sKSBvFrzvqe8",
        "colab": {}
      },
      "source": [
        "lossSerie10.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh00YJyVYvJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred10, volLocale10, dNN_T10, gNN_K10, lossSerie10 = create_eval_model(NNArchitectureConstrainedRawDupire,\n",
        "                                                                         scaledDataSet,\n",
        "                                                                         True,\n",
        "                                                                         hyperparameters,\n",
        "                                                                         modelName = \"regularizedConvexHybridDupireVolModel\")\n",
        "modelSummary(y_pred10, volLocale10, dNN_T10, gNN_K10, dataSet)\n",
        "impV10 = plotImpliedVol(y_pred10, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VADXbcv2vqBp",
        "colab": {}
      },
      "source": [
        "y_pred10Test, volLocale10Test, dNN_T10Test, gNN_K10Test, lossSerie10Test = create_eval_model(NNArchitectureConstrainedRawDupire,\n",
        "                                                                                             scaledDataSetTest,\n",
        "                                                                                             True,\n",
        "                                                                                             hyperparameters,\n",
        "                                                                                             modelName = \"regularizedConvexHybridDupireVolModel\")\n",
        "modelSummary(y_pred10Test, volLocale10Test, dNN_T10Test, gNN_K10Test, dataSetTest)\n",
        "impV10Test = plotImpliedVol(y_pred10Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2bBB7MRVXmli",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nl9Gq8s4o_DH"
      },
      "source": [
        "### Standard network with soft constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soE7FV4GpEFV",
        "colab": {}
      },
      "source": [
        "y_pred11, volLocale11, dNN_T11, gNN_K11, lossSerie11 = create_train_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                          scaledDataSet,\n",
        "                                                                          True,\n",
        "                                                                          hyperparameters,\n",
        "                                                                          modelName = \"regularizedConvexSoftDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "999cJ67npECl",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KRVyq4jRpEAJ",
        "colab": {}
      },
      "source": [
        "lossSerie11.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbu9gPcdYvJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred11, volLocale11, dNN_T11, gNN_K11, lossSerie11 = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                         scaledDataSet,\n",
        "                                                                         True,\n",
        "                                                                         hyperparameters,\n",
        "                                                                         modelName = \"regularizedConvexSoftDupireVolModel\")\n",
        "modelSummary(y_pred11, volLocale11, dNN_T11, gNN_K11, dataSet)\n",
        "impV11 = plotImpliedVol(y_pred11, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0HxLb3apD4l",
        "colab": {}
      },
      "source": [
        "y_pred11Test, volLocale11Test, dNN_T11Test, gNN_K11Test, lossSerie11Test = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                                             scaledDataSetTest,\n",
        "                                                                                             True,\n",
        "                                                                                             hyperparameters,\n",
        "                                                                                             modelName = \"regularizedConvexSoftDupireVolModel\")\n",
        "modelSummary(y_pred11Test, volLocale11Test, dNN_T11Test, gNN_K11Test, dataSetTest)\n",
        "impV11Test = plotImpliedVol(y_pred11Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pMVAzDEKXe2Z"
      },
      "source": [
        "### Hard constrained architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hLkTjQUOXkOj",
        "colab": {}
      },
      "source": [
        "y_pred12, volLocale12, dNN_T12, gNN_K12, lossSerie12 = create_train_model(NNArchitectureHardConstrainedDupire,\n",
        "                                                                          scaledDataSet,\n",
        "                                                                          True,\n",
        "                                                                          hyperparameters,\n",
        "                                                                          modelName = \"regularizedConvexHardDupireVolModel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n9EwiocWXkLX",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JQzB6KPmXkJB",
        "colab": {}
      },
      "source": [
        "lossSerie12.iloc[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDI9ydh3YvJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred12, volLocale12, dNN_T12, gNN_K12, lossSerie12 = create_eval_model(NNArchitectureHardConstrainedDupire,\n",
        "                                                                         scaledDataSet,\n",
        "                                                                         True,\n",
        "                                                                         hyperparameters,\n",
        "                                                                         modelName = \"regularizedConvexHardDupireVolModel\")\n",
        "modelSummary(y_pred12, volLocale12, dNN_T12, gNN_K12, dataSet)\n",
        "impV12 = plotImpliedVol(y_pred12, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2548YwGSXj68",
        "colab": {}
      },
      "source": [
        "y_pred12Test, volLocale12Test, dNN_T12Test, gNN_K12Test, lossSerie12Test = create_eval_model(NNArchitectureHardConstrainedDupire,\n",
        "                                                                                             scaledDataSetTest,\n",
        "                                                                                             True,\n",
        "                                                                                             hyperparameters,\n",
        "                                                                                             modelName = \"regularizedConvexHardDupireVolModel\")\n",
        "modelSummary(y_pred12Test, volLocale12Test, dNN_T12Test, gNN_K12Test, dataSetTest)\n",
        "impV12Test = plotImpliedVol(y_pred12Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xccdFPbTXj3n",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nj3ve-crXj1B",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mu8QA9CmXjyg",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gr7G5r7JXjrJ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TCvO25GfA2op",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DvLBkdlWjwzg"
      },
      "source": [
        "## Monte Carlo pricing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s8feoYmarxmj"
      },
      "source": [
        "### Monte Carlo with implied vol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N9KS6Vkro1Rg",
        "colab": {}
      },
      "source": [
        "nbTimeStep = 100\n",
        "nbPaths = 100000\n",
        "def MonteCarloPricerImplicit(S,\n",
        "                             Strike,\n",
        "                             Maturity,\n",
        "                             rSpline,\n",
        "                             divSpline,\n",
        "                             nbPaths,\n",
        "                             nbTimeStep,\n",
        "                             impliedVol):\n",
        "  time_grid = np.linspace(0, Maturity, int(nbTimeStep + 1))\n",
        "  timeStep = Maturity / nbTimeStep\n",
        "  gaussianNoise = np.random.normal(scale = np.sqrt(timeStep), size=(nbTimeStep, nbPaths))\n",
        "\n",
        "  logReturn = np.zeros((nbTimeStep + 1, nbPaths))\n",
        "  logReturn[0,:] = 0\n",
        "\n",
        "  for i in range(nbTimeStep) :\n",
        "      t = time_grid[i]\n",
        "\n",
        "      St = S0 * np.exp(logReturn[i,:])\n",
        "      volLocale = impliedVol\n",
        "\n",
        "      mu = rSpline(t) - divSpline(t)\n",
        "      drift = np.ones(nbPaths) * (mu - np.square(volLocale) / 2.0) \n",
        "      logReturn[i + 1, :] = logReturn[i,:] + drift * timeStep + gaussianNoise[i,:] * volLocale\n",
        "  SFinal = S0 * np.exp(logReturn[-1, :])\n",
        "  return np.mean(np.maximum(Strike - SFinal, 0))\n",
        "\n",
        "def MonteCarloPricerVectorizedImplicit(S,\n",
        "                                       dataSet,\n",
        "                                       rSpline,\n",
        "                                       divSpline,\n",
        "                                       nbPaths,\n",
        "                                       nbTimeStep):\n",
        "  func = lambda x : MonteCarloPricerImplicit(S, x[\"Strike\"], x[\"Maturity\"], riskCurvespline, divSpline, nbPaths, nbTimeStep, x[\"ImpliedVol\"])\n",
        "  return dataSet.apply(func, axis=1) * np.exp(-riskFreeIntegral(dataSet.index.get_level_values(\"Maturity\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q1E6lHkPpYXD",
        "colab": {}
      },
      "source": [
        "mcResRef = MonteCarloPricerVectorizedImplicit(S0[0],\n",
        "                                              dataSet,\n",
        "                                              riskCurvespline,\n",
        "                                              divSpline,\n",
        "                                              nbPaths,\n",
        "                                              nbTimeStep)\n",
        "mcResRef.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iKWRzDyTpYTh",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResRef, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H8MJ9XlYr2My"
      },
      "source": [
        "### Monte Carlo local volatility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3n8evyIxsB2W"
      },
      "source": [
        "#### Constant local volatility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnZBKekABhBm",
        "colab": {}
      },
      "source": [
        "def volLocaleTest(S, T):\n",
        "  return np.ones_like(S) * 0.23"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JdkQpONK4k7H",
        "colab": {}
      },
      "source": [
        "nbTimeStep = 100\n",
        "nbPaths = 100000\n",
        "def MonteCarloPricer(S, \n",
        "                     Strike, \n",
        "                     Maturity, \n",
        "                     rSpline, \n",
        "                     divSpline, \n",
        "                     nbPaths, \n",
        "                     nbTimeStep, \n",
        "                     volLocaleFunction):\n",
        "  time_grid = np.linspace(0, Maturity, int(nbTimeStep + 1))\n",
        "  timeStep = Maturity / nbTimeStep\n",
        "  gaussianNoise = np.random.normal(scale = np.sqrt(timeStep), size=(nbTimeStep, nbPaths))\n",
        "\n",
        "  logReturn = np.zeros((nbTimeStep + 1, nbPaths))\n",
        "  logReturn[0,:] = 0\n",
        "\n",
        "  for i in range(nbTimeStep) :\n",
        "      t = time_grid[i]\n",
        "\n",
        "      St = S0 * np.exp(logReturn[i,:])\n",
        "      volLocale = volLocaleFunction(St, np.ones(nbPaths) * t)\n",
        "\n",
        "      mu = rSpline(t) - divSpline(t)\n",
        "      drift = np.ones(nbPaths) * (mu - np.square(volLocale) / 2.0) \n",
        "      logReturn[i + 1, :] = logReturn[i,:] + drift * timeStep + gaussianNoise[i,:] * volLocale\n",
        "  SFinal = S0 * np.exp(logReturn[-1, :])\n",
        "  return np.mean(np.maximum(Strike - SFinal, 0))\n",
        "\n",
        "def MonteCarloPricerVectorized(S, \n",
        "                               dataSet,\n",
        "                               rSpline, \n",
        "                               divSpline, \n",
        "                               nbPaths, \n",
        "                               nbTimeStep, \n",
        "                               volLocaleFunction):\n",
        "  func = lambda x : MonteCarloPricer(S, x[\"Strike\"], x[\"Maturity\"], riskCurvespline, divSpline, nbPaths, nbTimeStep, volLocaleFunction)\n",
        "  return dataSet.apply(func, axis=1) * np.exp(-riskFreeIntegral(dataSet.index.get_level_values(\"Maturity\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYHvsquue2JO",
        "colab": {}
      },
      "source": [
        "mcResSigmaRef = MonteCarloPricerVectorized(S0[0],\n",
        "                                           dataSet,\n",
        "                                           riskCurvespline,\n",
        "                                           divSpline,\n",
        "                                           nbPaths,\n",
        "                                           nbTimeStep,\n",
        "                                           volLocaleTest)\n",
        "mcResSigmaRef.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0_1OTaLlm7UW",
        "colab": {}
      },
      "source": [
        "(mcResSigmaRef <= dataSet[\"Price\"]).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jJR0sXsUmCm2",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResSigmaRef, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ysLo9wxsTOj"
      },
      "source": [
        "#### Extracting neural local volatility\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MV4SmwctjbjS",
        "colab": {}
      },
      "source": [
        "def evalVolLocale(NNFactory,\n",
        "                  strikes,\n",
        "                  maturities,\n",
        "                  dataSet,\n",
        "                  hyperParameters,\n",
        "                  modelName = \"bestModel\"):\n",
        "    \n",
        "    hidden_nodes = hyperParameters[\"nbUnits\"] \n",
        "\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Strike = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
        "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
        "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                    Strike,\n",
        "                                                                                    Maturity,\n",
        "                                                                                    scaleTensor,\n",
        "                                                                                    strikeMinTensor,\n",
        "                                                                                    vegaRef,\n",
        "                                                                                    hyperParameters,\n",
        "                                                                                    IsTraining=False)# one hidden layer\n",
        "\n",
        "\n",
        "    price_pred_tensor_sc= tf.multiply(factorPrice,price_pred_tensor)\n",
        "    TensorList[0] = price_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList) \n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = strikes.shape[0]\n",
        "    changedVar = changeOfVariable(strikes, maturities)\n",
        "    scaledStrike = (changedVar[0]-minColFunction)/scF\n",
        "    dividendFactor = changedVar[1]\n",
        "\n",
        "    def createFeedDict(s, t, d):\n",
        "        batchSize = s.shape[0]\n",
        "        feedDict = {Strike : np.reshape(s, (batchSize,1)), \n",
        "                    Maturity : np.reshape(t, (batchSize,1)) ,  \n",
        "                    factorPrice : np.reshape(d, (batchSize,1)), \n",
        "                    vegaRef : np.ones((batchSize,1))}\n",
        "        return feedDict\n",
        "    \n",
        "    epochFeedDict = createFeedDict(scaledStrike, maturities, dividendFactor)\n",
        "    \n",
        "    saver.restore(sess, modelName)  \n",
        "\n",
        "    evalList = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    \n",
        "    return pd.Series(evalList[1].flatten(), index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bG_opQ7DynZC",
        "colab": {}
      },
      "source": [
        "strikeLow = dataSetTest[\"Strike\"].min()\n",
        "strikeUp = dataSetTest[\"Strike\"].max()\n",
        "strikeGrid = np.linspace(strikeLow, strikeUp, 1000)\n",
        "matLow = dataSetTest[\"Maturity\"].min()\n",
        "matUp = dataSetTest[\"Maturity\"].max()\n",
        "matGrid = np.linspace(matLow, matUp, 1000)\n",
        "volLocaleGrid = np.meshgrid(strikeGrid, matGrid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oawxG7p4oGUA",
        "colab": {}
      },
      "source": [
        "def interpolatedMCLocalVolatility(localVol, strikes, maturities):\n",
        "    strikeLocVol = np.ravel(localVol.index.get_level_values(\"Strike\").values)\n",
        "    maturityLocVol = np.ravel(localVol.index.get_level_values(\"Maturity\").values)\n",
        "    xym = np.vstack((strikeLocVol, maturityLocVol)).T\n",
        "    opts = {'balanced_tree': False, 'compact_nodes': False}\n",
        "    f =  interpolate.NearestNDInterpolator(xym,\n",
        "                                           localVol.values.flatten(), \n",
        "                                           rescale=True,\n",
        "                                           tree_options=opts)\n",
        "\n",
        "    strikePrice = strikes\n",
        "    maturityPrice = maturities\n",
        "    #func = lambda x : f(x[0],x[1])\n",
        "    coordinates =  np.array( f(strikePrice, maturityPrice) ).flatten() # np.array( list( map( func, zip(strikePrice, maturityPrice) ) ) ).flatten() \n",
        "    return pd.Series(coordinates, index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rRwNc1rgM28W"
      },
      "source": [
        "##### Standard network, soft constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QBw5bnz6QKTQ",
        "colab": {}
      },
      "source": [
        "def neuralVolLocale(s,t):\n",
        "  vLoc = evalVolLocale(NNArchitectureVanillaSoftDupire,\n",
        "                       s, t,\n",
        "                       dataSet,\n",
        "                       hyperparameters,\n",
        "                       modelName = \"regularizedConvexSoftDupireVolModel\")\n",
        "  return vLoc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZwCXR8lzaQa",
        "colab": {}
      },
      "source": [
        "volLocalInterp = neuralVolLocale(volLocaleGrid[0].flatten(), \n",
        "                                 volLocaleGrid[1].flatten())\n",
        "volLocalInterp.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OUVTy-tl9HBE",
        "colab": {}
      },
      "source": [
        "volLocalInterp2 = neuralVolLocale(dataSetTest.index.get_level_values(\"Strike\").values.flatten(), \n",
        "                                  dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
        "volLocalInterp2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m5rPNAOtNLGn",
        "colab": {}
      },
      "source": [
        "nnVolLocale = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Amwp9SY-D1o",
        "colab": {}
      },
      "source": [
        "nnVolLocale2 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp2, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPeQZ53f0L46",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-VTQGuG49sv_",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp2,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L_osDqp2mJbz",
        "colab": {}
      },
      "source": [
        "plotSerie(dataSetTest[\"locvol\"],\n",
        "          Title = 'Inteprolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sdUIUb_MwVJ"
      },
      "source": [
        "##### Hard constraint Regularized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4WESqBRzRGyt",
        "colab": {}
      },
      "source": [
        "def neuralVolLocaleHardReg(s,t):\n",
        "  vLoc = evalVolLocale(NNArchitectureHardConstrainedDupire,\n",
        "                       s, t,\n",
        "                       dataSet,\n",
        "                       hyperparameters,\n",
        "                       modelName = \"regularizedConvexHardDupireVolModel\")\n",
        "  return vLoc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "coZ0NmLduoR7",
        "colab": {}
      },
      "source": [
        "volLocalInterp3 = neuralVolLocaleHardReg(volLocaleGrid[0].flatten(),\n",
        "                                         volLocaleGrid[1].flatten())\n",
        "volLocalInterp3.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rlLZq1p6RXc-",
        "colab": {}
      },
      "source": [
        "volLocalInterp4 = neuralVolLocaleHardReg(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
        "                                         dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
        "volLocalInterp4.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bHQgKjjMRXZe",
        "colab": {}
      },
      "source": [
        "nnVolLocale3 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp3, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eMy6YZzmRXXX",
        "colab": {}
      },
      "source": [
        "nnVolLocale4 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp4, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bl72KtNERXT_",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp3,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ILzOqJ0SRXOE",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp4,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9167YBhRpIY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_RU2VfS9UcAN"
      },
      "source": [
        "##### Hard constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fdam7WpUcAS",
        "colab": {}
      },
      "source": [
        "def neuralVolLocaleHard(s,t):\n",
        "  vLoc = evalVolLocale(NNArchitectureHardConstrainedDupire,\n",
        "                       s, t,\n",
        "                       dataSet,\n",
        "                       hyperparameters,\n",
        "                       modelName = \"convexHardDupireVolModel\")\n",
        "  return vLoc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "42NAiQ7OUcAd",
        "colab": {}
      },
      "source": [
        "volLocalInterp5 = neuralVolLocaleHard(volLocaleGrid[0].flatten(),\n",
        "                                      volLocaleGrid[1].flatten())\n",
        "volLocalInterp5.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1NZ9rNwyUcAj",
        "colab": {}
      },
      "source": [
        "volLocalInterp6 = neuralVolLocaleHard(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
        "                                      dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
        "volLocalInterp6.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UjY8_upSUcAn",
        "colab": {}
      },
      "source": [
        "nnVolLocale5 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp5, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A2uShCIhUcAr",
        "colab": {}
      },
      "source": [
        "nnVolLocale6 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp6, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yDLVxVCsUcAu",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp5,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4F5jBRtdUcAw",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp6,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZojUS1lnUcAz",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CslgrmvEUcA1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w96mzVNxRpFw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LIWdTVy8tf59"
      },
      "source": [
        "#### Tikhonov local volatility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "odpuxjDbtlV0",
        "colab": {}
      },
      "source": [
        "def interpolatedMCTikhonov(localVol, strikes, maturities):\n",
        "    strikeLocVol = np.ravel(localVol.index.get_level_values(\"Strike\").values)\n",
        "    maturityLocVol = np.ravel(localVol.index.get_level_values(\"Maturity\").values)\n",
        "    xym = np.vstack((strikeLocVol, maturityLocVol)).T\n",
        "    opts = {'balanced_tree': False, 'compact_nodes': False}\n",
        "    f =  interpolate.NearestNDInterpolator(xym,\n",
        "                                           localVol.values.flatten(), \n",
        "                                           rescale=True,\n",
        "                                           tree_options=opts)\n",
        "\n",
        "    strikePrice = strikes\n",
        "    maturityPrice = maturities\n",
        "    #func = lambda x : f(x[0],x[1])\n",
        "    coordinates =  np.array( f(strikePrice, maturityPrice) ).flatten() # np.array( list( map( func, zip(strikePrice, maturityPrice) ) ) ).flatten() \n",
        "    return pd.Series(coordinates, index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))\n",
        "\n",
        "nnTikhonov = lambda x,y : interpolatedMCTikhonov(dataSetTest[\"locvol\"], x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cP0LG52vtlQ5",
        "colab": {}
      },
      "source": [
        "mcResTikhonov = MonteCarloPricerVectorized(S0[0],\n",
        "                                           dataSet,\n",
        "                                           riskCurvespline,\n",
        "                                           divSpline,\n",
        "                                           nbPaths,\n",
        "                                           nbTimeStep,\n",
        "                                           nnTikhonov)\n",
        "mcResTikhonov.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uH2OxQ0OtlNd",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResTikhonov, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t65wX7-xtlKU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSe4AmUJsLo-"
      },
      "source": [
        "#### Neural local Volatility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z3Hrt5KOVt9S"
      },
      "source": [
        "##### Standard Network soft constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T0DdvwGtdKPZ",
        "colab": {}
      },
      "source": [
        "mcResVolLocale = MonteCarloPricerVectorized(S0[0],\n",
        "                                            dataSet,\n",
        "                                            riskCurvespline,\n",
        "                                            divSpline,\n",
        "                                            nbPaths,\n",
        "                                            nbTimeStep,\n",
        "                                            nnVolLocale)\n",
        "mcResVolLocale.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAslenyljg_w",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "38OJqpkCfYaI",
        "colab": {}
      },
      "source": [
        "dataSet.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0eW1ICq6-RZ3",
        "colab": {}
      },
      "source": [
        "mcResVolLocale2 = MonteCarloPricerVectorized(S0[0],\n",
        "                                            dataSet,\n",
        "                                            riskCurvespline,\n",
        "                                            divSpline,\n",
        "                                            nbPaths,\n",
        "                                            nbTimeStep,\n",
        "                                            nnVolLocale2)\n",
        "mcResVolLocale2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jNmkLtZE-RPA",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale2, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "demJXGlD6V3U",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pwupw4shWcqI"
      },
      "source": [
        "##### Hard constraint Regularized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HtFZpARdWcqN",
        "colab": {}
      },
      "source": [
        "mcResVolLocale3 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSet,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale3)\n",
        "mcResVolLocale3.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4JQvapLCWcqS",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale3, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RbgkiRPeWcqY",
        "colab": {}
      },
      "source": [
        "mcResVolLocale4 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSet,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale4)\n",
        "mcResVolLocale4.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sb6dt-iJWcqa",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale4, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FoR4evV3Wcqd",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z45IZtNnWfek"
      },
      "source": [
        "##### Hard constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2vCtUGRNWfel",
        "colab": {}
      },
      "source": [
        "mcResVolLocale5 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSet,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale5)\n",
        "mcResVolLocale5.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1h3Ee3CcWfen",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale5, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XudXGmowWffz",
        "colab": {}
      },
      "source": [
        "mcResVolLocale6 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSet,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale6)\n",
        "mcResVolLocale6.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oz5Rx-INWff1",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale6, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V4XXpLmjWff3",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q-FhHmEUAaIj"
      },
      "source": [
        "## Hyperparameter selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CDIM91VNAk-s",
        "colab": {}
      },
      "source": [
        "def selectHyperparameters(hyperparameters, parameterOfInterest, modelFactory, modelName, activateDupireReg, logGrid = True):\n",
        "    oldValue = hyperparameters[parameterOfInterest]\n",
        "    gridValue = oldValue * ( np.exp( np.log(10) * np.array([-2,-1, 0, 1, 2])) if logGrid else np.array([0.2, 0.5, 1, 2, 5]) )\n",
        "    \n",
        "    oldNbEpochs = hyperparameters[\"maxEpoch\"]\n",
        "    hyperparameters[\"maxEpoch\"] = int(oldNbEpochs / 10)\n",
        "    trainLoss = {}\n",
        "    arbitrageViolation = {}\n",
        "    for v in gridValue :\n",
        "        hyperparameters[parameterOfInterest] = int(v)\n",
        "        pred, volLoc, theta, gammaK, loss = create_train_model(modelFactory,\n",
        "                                                               scaledDataSet,\n",
        "                                                               activateDupireReg,\n",
        "                                                               hyperparameters,\n",
        "                                                               modelName = modelName)\n",
        "        nbArbitrageViolation = np.sum((theta <= 0)) + np.sum((gammaK <= 0))\n",
        "        trainLoss[v] = min(loss)\n",
        "        arbitrageViolation[v] = nbArbitrageViolation\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "    hyperparameters[\"maxEpoch\"] = oldNbEpochs\n",
        "    hyperparameters[parameterOfInterest] = oldValue\n",
        "    # Plot curves\n",
        "    \n",
        "    fig, ax1 = plt.subplots()\n",
        "    if logGrid :\n",
        "        plt.xscale('symlog')\n",
        "    \n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Value')\n",
        "    ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(pd.Series(trainLoss), color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "    \n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('Arbitrage violation', color=color)  # we already handled the x-label with ax1\n",
        "    ax2.plot(pd.Series(arbitrageViolation), color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "    plt.show()\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "REnyuzq2Ak5V",
        "colab": {}
      },
      "source": [
        "def selectHyperparametersRandom(hyperparameters, \n",
        "                                parametersOfInterest, \n",
        "                                modelFactory, \n",
        "                                modelName, \n",
        "                                activateDupireReg, \n",
        "                                nbAttempts,\n",
        "                                logGrid = True):\n",
        "    oldValue = {} \n",
        "    for k in parametersOfInterest :\n",
        "        oldValue[k] = hyperparameters[k]\n",
        "    \n",
        "    gridValue = np.exp( np.log(10) * np.array([-2,-1, 0, 1, 2])) if logGrid else np.array([0.2, 0.5, 1, 2, 5]) \n",
        "    \n",
        "    oldNbEpochs = hyperparameters[\"maxEpoch\"]\n",
        "    hyperparameters[\"maxEpoch\"] = int(oldNbEpochs / 10)\n",
        "    trainLoss = {}\n",
        "    arbitrageViolation = {}\n",
        "    nbTry = nbAttempts\n",
        "    for v in range(nbTry) :\n",
        "        combination = np.random.randint(5, size = len(parametersOfInterest) )\n",
        "        for p in range(len(parametersOfInterest)):\n",
        "            hyperparameters[parametersOfInterest[p]] = oldValue[parametersOfInterest[p]] * gridValue[int(combination[p])]\n",
        "            print(parametersOfInterest[p] , \" : \", hyperparameters[parametersOfInterest[p]])\n",
        "        pred, volLoc, theta, gammaK, loss = create_train_model(modelFactory,\n",
        "                                                               scaledDataSet,\n",
        "                                                               activateDupireReg,\n",
        "                                                               hyperparameters,\n",
        "                                                               modelName = modelName)\n",
        "        nbArbitrageViolation = np.sum((theta <= 0)) + np.sum((gammaK <= 0))\n",
        "        print(\"loss : \", min(loss))\n",
        "        print(\"nbArbitrageViolation : \", nbArbitrageViolation)\n",
        "        print()\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "    hyperparameters[\"maxEpoch\"] = oldNbEpochs\n",
        "    for k in parametersOfInterest :\n",
        "        hyperparameters[k] = oldValue[k]\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0P-fv_wMAk2y",
        "colab": {}
      },
      "source": [
        "selectHyperparametersRandom(hyperparameters,\n",
        "                            [\"lambdaLocVol\",\"lambdaSoft\",\"lambdaGamma\"],\n",
        "                            NNArchitectureConstrainedRawDupire,\n",
        "                            \"hyperParameters\",\n",
        "                            True, \n",
        "                            100,\n",
        "                            logGrid = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eWz86KbKCqa-",
        "colab": {}
      },
      "source": [
        "\n",
        "hyperparameters[\"lambdaLocVol\"] = 100\n",
        "hyperparameters[\"lambdaSoft\"] = 100 \n",
        "hyperparameters[\"lambdaGamma\"] = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1jSAmMeZ6V15",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"lambdaLocVol\", \n",
        "                      NNArchitectureVanillaSoftDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dANDwRU6DEom",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"DupireVarCap\", \n",
        "                      NNArchitectureConstrainedRawDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8-j6a1jpFt1_",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"lambdaLocVol\", \n",
        "                      NNArchitectureUnconstrainedDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_thp08oSGj6m",
        "colab": {}
      },
      "source": [
        "hyperparameters[\"lambdaLocVol\"] = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Smxq-gfiGGV8",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"lambdaLocVol\", \n",
        "                      NNArchitectureConstrainedRawDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GPSNSuGIGsMC",
        "colab": {}
      },
      "source": [
        "hyperparameters[\"nbUnits\"] = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UQb2l-ZqIKyC",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"nbUnits\", \n",
        "                      NNArchitectureVanillaSoftDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_W3QYBDuMSu-",
        "colab": {}
      },
      "source": [
        "hyperparameters[\"nbUnits\"] = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q-lYydHFB8G1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}